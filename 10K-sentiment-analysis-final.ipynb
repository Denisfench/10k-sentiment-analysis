{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h2>S&P 500 Sentiment Analysis Based on the \"When Is a Liability Not a\n",
    "Liability?\n",
    " Textual Analysis, Dictionaries, and 10-Ks\" work by TIM LOUGHRAN and BILL\n",
    " MCDONALD.<h2>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import project packages\n",
    "import wrds\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sec_edgar_downloader import Downloader\n",
    "import time\n",
    "import calendar\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pysentiment2 as ps\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function retrieves all sp500 records from the WRDS database\n",
    "# the records are returned in Pandas dataframe format\n",
    "def get_sp500_records():\n",
    "    WRDS_USERNAME = 'denisfench'\n",
    "    # establish connection to the WRDS database\n",
    "    conn = wrds.Connection(wrds_username=WRDS_USERNAME)\n",
    "\n",
    "    ### Get S&P500 Index Membership from CRSP\n",
    "    sp500 = conn.raw_sql(\"\"\"\n",
    "                            select a.*, b.date, b.ret\n",
    "                            from crsp.msp500list as a,\n",
    "                            crsp.msf as b\n",
    "                            where a.permno=b.permno\n",
    "                            and b.date >= a.start and b.date<= a.ending\n",
    "                            and b.date>='01/01/2000'\n",
    "                            order by date;\n",
    "                            \"\"\", date_cols=['start', 'ending', 'date'])\n",
    "\n",
    "    ### Add Other Company Identifiers from CRSP.MSENAMES, such as TICKER, SHRCD, EXCHCD\n",
    "    mse = conn.raw_sql(\"\"\"\n",
    "                            select comnam, ncusip, namedt, nameendt,\n",
    "                            permno, shrcd, exchcd, hsiccd, ticker\n",
    "                            from crsp.msenames\n",
    "                            \"\"\", date_cols=['namedt', 'nameendt'])\n",
    "\n",
    "    # if nameendt is missing then set to today date\n",
    "    mse['nameendt']=mse['nameendt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "    # Merge with SP500 data\n",
    "    sp500_full = pd.merge(sp500, mse, how = 'left', on = 'permno')\n",
    "\n",
    "    # Impose the date range restrictions\n",
    "    sp500_full = sp500_full.loc[(sp500_full.date>=sp500_full.namedt) \\\n",
    "                                & (sp500_full.date<=sp500_full.nameendt)]\n",
    "\n",
    "    ### Add Other Company Identifiers from CRSP.MSENAMES\n",
    "    mse = conn.raw_sql(\"\"\"\n",
    "                            select comnam, ncusip, namedt, nameendt,\n",
    "                            permno, shrcd, exchcd, hsiccd, ticker\n",
    "                            from crsp.msenames\n",
    "                            \"\"\", date_cols=['namedt', 'nameendt'])\n",
    "\n",
    "    # if nameendt is missing then set to today date\n",
    "    mse['nameendt']=mse['nameendt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "    # Merge with SP500 data\n",
    "    sp500_full = pd.merge(sp500, mse, how = 'left', on = 'permno')\n",
    "\n",
    "    # Impose the date range restrictions\n",
    "    sp500_full = sp500_full.loc[(sp500_full.date>=sp500_full.namedt) \\\n",
    "                                & (sp500_full.date<=sp500_full.nameendt)]\n",
    "\n",
    "    ### Add Compustat Identifiers\n",
    "    ccm=conn.raw_sql(\"\"\"\n",
    "                      select gvkey, liid as iid, lpermno as permno,\n",
    "                      linktype, linkprim, linkdt, linkenddt\n",
    "                      from crsp.ccmxpf_linktable\n",
    "                      where substr(linktype,1,1)='L'\n",
    "                      and (linkprim ='C' or linkprim='P')\n",
    "                      \"\"\", date_cols=['linkdt', 'linkenddt'])\n",
    "\n",
    "    # if linkenddt is missing then set to today date\n",
    "    ccm['linkenddt']=ccm['linkenddt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "    # Merge the CCM data with S&P500 data\n",
    "    # First just link by matching PERMNO\n",
    "    sp500ccm = pd.merge(sp500_full, ccm, how='left', on=['permno'])\n",
    "\n",
    "    # Then set link date bounds\n",
    "    sp500ccm = sp500ccm.loc[(sp500ccm['date']>=sp500ccm['linkdt'])\\\n",
    "                            &(sp500ccm['date']<=sp500ccm['linkenddt'])]\n",
    "\n",
    "    # Rearrange columns for final output\n",
    "    sp500ccm = sp500ccm.drop(columns=['namedt', 'nameendt', 'linktype', \\\n",
    "                                      'linkprim', 'linkdt', 'linkenddt'])\n",
    "    sp500ccm = sp500ccm[['date', 'permno', 'comnam', 'ncusip',\\\n",
    "                         'shrcd', 'exchcd', 'hsiccd', 'ticker', \\\n",
    "                         'gvkey', 'iid', 'start', 'ending', 'ret']]\n",
    "\n",
    "    ### Add CIKs and Link with SEC Index Files using CIK\n",
    "    names = conn.raw_sql(\"\"\" select gvkey, cik, sic, naics, gind, gsubind from comp.names \"\"\")\n",
    "\n",
    "    # Merge sp500 constituents table with names table\n",
    "    sp500 = pd.merge(sp500ccm, names, on='gvkey',  how='left')\n",
    "    return sp500"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function retrieves a subsample of the sp500 records corresponding to\n",
    "# the given dates (YYYY-MM-DD)\n",
    "# EX: get_sp500_records_by_date(sp500, '2020-01-01', '2021-12-31')\n",
    "\n",
    "def get_sp500_records_by_date(sp500_record, start_date, end_date):\n",
    "    new_sp500_sample = sp500_record.loc[start_date <= sp500_record\n",
    "        .date][['date',\n",
    "                                                                    'permno',\n",
    "                                                      'comnam',\n",
    "                                               'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "\n",
    "    new_sp500_sample = new_sp500_sample.loc[sp500_record.date <=\n",
    "                                      end_date][['date',\n",
    "                                                                        'permno',\n",
    "                                                          'comnam',\n",
    "                                                   'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "\n",
    "    return new_sp500_sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# increment the current date by 1 month\n",
    "def get_next_month(date, DATE_FORMAT=\"%Y-%m-%d\", datetime_format=False):\n",
    "    datetime_date = datetime.strptime(date, DATE_FORMAT)\n",
    "    new_date = datetime_date + relativedelta(months=1)\n",
    "    if datetime_format:\n",
    "        return new_date\n",
    "    else:\n",
    "        return new_date.strftime(DATE_FORMAT)\n",
    "\n",
    "\n",
    "# increment the current date by 1 day\n",
    "def get_next_day(date, datetime_format=False):\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    datetime_date = datetime.strptime(date, DATE_FORMAT)\n",
    "    new_date = datetime_date + relativedelta(days=1)\n",
    "    if datetime_format:\n",
    "        return new_date\n",
    "    else:\n",
    "        return new_date.strftime(DATE_FORMAT)\n",
    "\n",
    "\n",
    "# get the last day of the current month\n",
    "def get_end_month_date(curr_date, datetime_format=False):\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    datetime_date = datetime.strptime(curr_date, DATE_FORMAT)\n",
    "    current_date = curr_date\n",
    "    month_end_date = current_date[:8] + str(calendar.monthrange(datetime_date.year,\n",
    "                                                datetime_date\n",
    "                                      .month)[1])\n",
    "    month_end_date = datetime.strptime(month_end_date, DATE_FORMAT)\n",
    "    if datetime_format:\n",
    "        return month_end_date\n",
    "    else:\n",
    "        return month_end_date.strftime(DATE_FORMAT)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function retrieves all 10-K and 10-Q fillings for a given company for\n",
    "# a given month\n",
    "def get_filing_by_month(ciks, date, dir_name):\n",
    "    num_filings = 0\n",
    "    dl = Downloader(dir_name)\n",
    "    for cik in ciks:\n",
    "        num_filings += dl.get(\"10-Q\", cik, after=date, before=get_next_month\n",
    "        (date))\n",
    "        num_filings += dl.get(\"10-K\", cik, after=date, before=get_next_month\n",
    "        (date))\n",
    "    return num_filings\n",
    "\n",
    "# this function retrieves all 10-K and 10-Q fillings for a given company for\n",
    "# a given day\n",
    "def get_filing_by_day(ciks, date, dir_name):\n",
    "    num_filings = 0\n",
    "    dl = Downloader(dir_name)\n",
    "    for cik in ciks:\n",
    "        num_filings += dl.get(\"10-Q\", cik, after=date, before=get_next_day\n",
    "        (date))\n",
    "        num_filings += dl.get(\"10-K\", cik, after=date, before=get_next_day\n",
    "        (date))\n",
    "    return num_filings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function retrieves all S&P 500 10-Q or 10-K filings for a given month,\n",
    "# if such a filing exists for a given company and writes them to a given directory\n",
    "# the date should be passed in the following format: YYYY-MM-DD\n",
    "# the company lookup is based in the cik number by default\n",
    "def get_all_sp_filings_by_month(sp500_collection, date, dir_name,\n",
    "                                identifier='cik'):\n",
    "    sp500_ids = []\n",
    "    num_filings = 0\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    current_date = date\n",
    "    month_end_date = get_end_month_date(current_date, datetime_format=True)\n",
    "\n",
    "    while datetime.strptime(current_date, DATE_FORMAT) <= month_end_date:\n",
    "        sp500_companies = sp500_collection.loc[sp500_collection.date ==\n",
    "                                               current_date][['date',\n",
    "                                                                        'permno',\n",
    "                                                          'comnam',\n",
    "                                                   'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "\n",
    "        sp500_ids.extend(sp500_companies[identifier].tolist())\n",
    "        current_date = get_next_day(current_date)\n",
    "\n",
    "    num_filings += get_filing_by_month(sp500_ids,\n",
    "                                       current_date,\n",
    "                                       dir_name)\n",
    "\n",
    "    return num_filings\n",
    "\n",
    "# this function retrieves all S&P 500 10-Q or 10-K filings for a given day,\n",
    "# if such a filing exists for a given company and writes them to a given directory\n",
    "# the date should be passed in the following format: YYYY-MM-DD\n",
    "def get_all_sp_filings_by_day(sp500_collection, date, dir_name, identifier='cik'):\n",
    "    num_filings = 0\n",
    "    sp500_companies = sp500_collection.loc[sp500_collection.date == date][['date',\n",
    "                                                                    'permno',\n",
    "                                                      'comnam',\n",
    "                                               'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "    sp500_tickers = sp500_companies[identifier]\n",
    "\n",
    "    for ticker in sp500_tickers:\n",
    "        num_filings += get_filing_by_day(ticker, date, dir_name)\n",
    "\n",
    "    return num_filings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function retrieves all S&P 500 10-Q and 10-K filings for a given period\n",
    "def get_sp500_filings(sp500_collection, filings_start_date, filings_end_date,\n",
    "                      dir_name):\n",
    "\n",
    "    print(\"Retrieving all the 10-Q and 10-K filings starting from \",\n",
    "           filings_start_date, \" until \", filings_end_date)\n",
    "\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    current_date = filings_start_date\n",
    "    num_filings = 0\n",
    "    while datetime.strptime(current_date, DATE_FORMAT) <= datetime.strptime\\\n",
    "                (filings_end_date, DATE_FORMAT):\n",
    "        num_filings += get_all_sp_filings_by_month(sp500_collection, current_date,\n",
    "                                                   dir_name)\n",
    "        current_date = get_next_month(current_date)\n",
    "\n",
    "        print(\"Successfully retrieved \" + str(num_filings) + \" for \" + str\n",
    "        (current_date))\n",
    "\n",
    "    return num_filings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function retrieves all tickers or ciks for all companies from a given\n",
    "# directory\n",
    "def get_all_filing_ids(dir_name):\n",
    "    ids = os.listdir(dir_name + \"/sec-edgar-filings\")\n",
    "    return ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function extracts the filing date, given the filing date line from the\n",
    "# filing document\n",
    "def extract_filing_date(filed_as_of_date_line):\n",
    "    DATE_FORMAT = \"%Y%m%d\"\n",
    "    date = \"\"\n",
    "    for ch in filed_as_of_date_line:\n",
    "        if ch.isdigit():\n",
    "            date += ch\n",
    "    return datetime.strptime(date, DATE_FORMAT).date()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FILING_DETAILS = \"filing-details.html\"\n",
    "FULL_SUBMISSION = \"full-submission.txt\"\n",
    "FILED_AS_OF_DATE_IDX = 7\n",
    "\n",
    "# this function will search for a filing in a given directory based on the\n",
    "# given ticker or cik\n",
    "# it will return a dictionary with a ticker and the date of filing as the key\n",
    "# and filing text as the value\n",
    "# currently the function doesn't indicate whether the filing is a 10-Q or 10-K\n",
    "def get_filings_from_id(comp_id, dir_name, sec_filings):\n",
    "    for root, dirs, files in os.walk(dir_name + \"/sec-edgar-filings/\" + comp_id):\n",
    "        for f in files:\n",
    "            if FILING_DETAILS in f:\n",
    "                with open(root + \"/\" + f) as fp:\n",
    "                    soup = BeautifulSoup(fp, \"html.parser\")\n",
    "                    line_idx = 0\n",
    "                    with open(root + \"/\" + FULL_SUBMISSION) as fs_fp:\n",
    "                        while line_idx < FILED_AS_OF_DATE_IDX:\n",
    "                            line_idx += 1\n",
    "                            fs_fp.readline()\n",
    "                        date = extract_filing_date(fs_fp.readline())\n",
    "                    sec_filings[(comp_id, date)] = soup.get_text()\n",
    "    return sec_filings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get filing body for all filings in the list containing filing ids\n",
    "def get_filings_from_comp_ids(ids, dir_name):\n",
    "    sec_filings = {}\n",
    "    for comp_id in ids:\n",
    "        # sec_filings[comp_id] = get_filings_from_id(comp_id, dir_name, sec_filings)\n",
    "        get_filings_from_id(comp_id, dir_name, sec_filings)\n",
    "    return sec_filings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# this function sets up SSL context for downloading packages from nltk library\n",
    "def create_ssl_context():\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# a function that tokenizes 10-K or 10-Q corpus that\n",
    "def tokenize_filing(filing_corpus):\n",
    "    filing_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return filing_tokenizer.tokenize(filing_corpus)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# a function to remove the stop words from the filings corpus\n",
    "def filter_out_stopwords(tokenized_filing_corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filing_corpus_filtered = [word for word in tokenized_filing_corpus if not \\\n",
    "        word.lower() in stop_words]\n",
    "    return filing_corpus_filtered\n",
    "\n",
    "\n",
    "# a function that filters out numbers from a filing corpus\n",
    "def filter_out_numbers(filing_corpus):\n",
    "    return [token for token in filing_corpus if not (token.isdigit()\n",
    "                                         or token[0] == '-' and token[1:].isdigit())]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function filters out both the stop words and the numbers\n",
    "def clean_filing_corpus(tokenized_filing_corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filing_corpus_filtered = [word for word in tokenized_filing_corpus if not \\\n",
    "        word.lower() in stop_words and word if not (word.isdigit()\n",
    "                                         or word[0] == '-' and word[1:].isdigit())]\n",
    "    return filing_corpus_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on 10-Q and 10-K filing documents\n",
    "\n",
    "# this function calculates the sentiment score of the text based on a\n",
    "# sentiment dictionary (Loughran or Harvard), passed in as a parameter\n",
    "def get_sentiment_score(text, dictionary):\n",
    "\n",
    "    # get the Harvard general sentiment dictionary\n",
    "    hiv4 = ps.HIV4()\n",
    "\n",
    "    # get the Loughran and McDonald dictionary\n",
    "    lm = ps.LM()\n",
    "\n",
    "    if dictionary == \"Harvard\":\n",
    "        return hiv4.get_score(text)\n",
    "\n",
    "    if dictionary == \"Loughran\":\n",
    "        return lm.get_score(text)\n",
    "\n",
    "    raise ValueError(\"The dictionary could not be found.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function retrieves Value-Weighted Return (including distributions) and\n",
    "# Holding Period Return from the CRSP database\n",
    "def get_crsp_data():\n",
    "    CRSP_DATA_DIR = \"CRSP-data/\"\n",
    "    crsp_df = pd.read_csv(CRSP_DATA_DIR + \"crsp_2020_2021.csv\", parse_dates=['date'])\n",
    "    return crsp_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PERMNO_IDX = 2\n",
    "\n",
    "def get_permno_from_ticker(sp500, ticker):\n",
    "    return int(sp500.loc[sp500['ticker'] == ticker, 'permno'].iloc[PERMNO_IDX])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function calculates excess return using Value-Weighted Return (including distributions) and\n",
    "# Holding Period Return from the CRSP database\n",
    "def get_excess_return(crsp_dataframe, permno, date):\n",
    "    excess_return_df = crsp_dataframe[crsp_dataframe['PERMNO'] == permno]\n",
    "    excess_return_df =  excess_return_df[crsp_dataframe['date'] == date]\n",
    "    excess_return = float(excess_return_df['RET']) - float\\\n",
    "(excess_return_df['vwretd'])\n",
    "    return excess_return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DICTIONARY_DIR = \"dictionaries/\"\n",
    "\n",
    "# this function constructs a sentiment dictionary based on the Loughran\n",
    "# McDonald and Harvard dictionaries\n",
    "# return: {Term: [str], Sentiment: [int], Term_Count: [int]}\n",
    "# where Term count is the number of times the term occurs within the corpus\n",
    "def construct_sentiment_dict(base_dict=\"Loughran\"):\n",
    "    if base_dict == \"Loughran\":\n",
    "        df = pd.read_csv(DICTIONARY_DIR +\n",
    "                         \"Loughran-McDonald_MasterDictionary_1993-2021.csv\",\n",
    "                         header=0, usecols=['Word', 'Negative'])\n",
    "        df = df[(df['Negative'] != 0)]\n",
    "\n",
    "    else:\n",
    "        df = pd.read_csv(DICTIONARY_DIR + \"Harvard_Negative_Word_List.txt\",\n",
    "                         sep='/n')\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This function constructs a term dictionary based on a given document: tokenized filing list\n",
    "# return: {Term: [str], Term_Count: [int]}\n",
    "# where Term count is the number of times the term occurs within the document\n",
    "def construct_term_dict(document):\n",
    "    doc_dict = {}\n",
    "    if isinstance(document, str):\n",
    "        document_tokens = document.split()\n",
    "    else:\n",
    "        document_tokens = document\n",
    "    for token in document_tokens:\n",
    "        token = ' '.join(token)\n",
    "        if token in doc_dict:\n",
    "            doc_dict[token] += 1\n",
    "        else:\n",
    "            doc_dict[token] = 1\n",
    "    return doc_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This function calculates a document negativity score based on the\n",
    "# proportion of negative words in the document\n",
    "# the negativity of the term is determined based on a given dictionary\n",
    "# (Loughran or Harvard)\n",
    "def compute_negativity_score_based_on_proportion(text,\n",
    "                                                 sentiment_dict=\"Loughran\"):\n",
    "    sentiment_score = get_sentiment_score(text, sentiment_dict)\n",
    "    num_negative = sentiment_score['Negative']\n",
    "    return num_negative / len(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This function calculates TFIDF score for the filings based on the sentiment\n",
    "# dictionary vocabulary using SciKit Learn\n",
    "def calc_tfidf_score(corpus_of_filings):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "                             stop_words=\"english\")\n",
    "    vectors = vectorizer.fit_transform(corpus_of_filings)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    dense_list = dense.tolist()\n",
    "    tfidf_df = pd.DataFrame(dense_list, columns=feature_names)\n",
    "    return tfidf_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function loads all the filings from a given directory, which\n",
    "# constitute an entire corpus of documents\n",
    "def load_filings_corpus(corpus_root_dir):\n",
    "    FILING_DETAILS = \"filing-details.html\"\n",
    "    sec_filings_corpus = []\n",
    "    for root, dirs, files in os.walk(corpus_root_dir + \"/sec-edgar-filings/\"):\n",
    "        for f in files:\n",
    "            if FILING_DETAILS in f:\n",
    "                with open(root + \"/\" + f) as file_ptr:\n",
    "                    bs = BeautifulSoup(file_ptr, \"html.parser\")\n",
    "                    sec_filings_corpus.append(bs.get_text())\n",
    "    return sec_filings_corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function plots negativity score (quantile) vs median excess return for a\n",
    "# sample\n",
    "# of companies\n",
    "\n",
    "def plot_negativity_vs_median_excess_return\\\n",
    "(quantile, median_filing_period_excess_return, label):\n",
    "    plt.xlabel('Quintile (based on proportion of negative words)')\n",
    "    plt.ylabel('Median Filling Period Excess Return')\n",
    "    plt.plot(quantile, median_filing_period_excess_return\n",
    "             , label=label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>SENTIMENT ANALYSIS OF 10-Qs and 10-Ks for S&P 500 companies for 2020,\n",
    "2021<h1>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_ssl_context()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# getting all S&P 500 records from the WRDS database\n",
    "\n",
    "sp500_all = get_sp500_records()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# retrieving all S&P 500 records for the two-year period (from '2020-01-01'\n",
    "# to '2021-12-31')\n",
    "\n",
    "sp500_sample = get_sp500_records_by_date(sp500_all, \"2020-01-01\", \"2021-12-31\")\n",
    "sp500_sample.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get a sample of S&P 500 filings for a 2-year period\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2021-12-31\"\n",
    "sp_filings_dir_name = 'sp500_filings_sample'\n",
    "num_filings_sample = get_sp500_filings(sp500_sample, start_date, end_date, sp_filings_dir_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "FILINGS_SAMPLE_DIR = \"data\"\n",
    "# retrieving tickers of companies who's filings we have in our file system\n",
    "sample_ids = get_all_filing_ids(FILINGS_SAMPLE_DIR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading serialized version...\n"
     ]
    }
   ],
   "source": [
    "load_serialized_version = True\n",
    "SERIALIZATION_DIR = \"serialized/\"\n",
    "\n",
    "sample_filings_corpus = {}\n",
    "\n",
    "if (load_serialized_version):\n",
    "    print(\"Loading serialized version...\")\n",
    "    try:\n",
    "        sample_filings_corpus = pickle.load(open(SERIALIZATION_DIR +\n",
    "                                                 \"sample_filings_corpus\"\n",
    "                                                 \".pickle\", \"rb\"))\n",
    "    except (OSError, IOError) as e:\n",
    "        print(\"Serialized version was not found. Retrieving filings and \"\n",
    "              \"creating a serialized file.\")\n",
    "        sample_filings_corpus = get_filings_from_comp_ids(sample_ids,\n",
    "                                                        FILINGS_SAMPLE_DIR)\n",
    "        pickle.dump(sample_filings_corpus, open(SERIALIZATION_DIR +\n",
    "                                                \"sample_filings_corpus\"\n",
    "                                                \".pickle\", \"wb\"))\n",
    "        print(\"Done!\")\n",
    "\n",
    "else:\n",
    "    sample_filings_corpus = get_filings_from_comp_ids(sample_ids, FILINGS_SAMPLE_DIR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and cleaning the filings...\n"
     ]
    }
   ],
   "source": [
    "# tokenize and clean the body of the filings\n",
    "print(\"Tokenizing and cleaning the filings...\")\n",
    "\n",
    "for filing in sample_filings_corpus:\n",
    "    filing_text = sample_filings_corpus[filing]\n",
    "    # extract a list of tokens from the filing corpus\n",
    "    tokenized_filing = tokenize_filing(filing_text)\n",
    "    # remove the stop words and digits from the tokenized filing\n",
    "    tokenized_filing = clean_filing_corpus(tokenized_filing)\n",
    "    sample_filings_corpus[filing] = tokenized_filing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculating sentiment score of each filing based on the proportion of\n",
    "# negative words present in it based on Loughran and Harvard dictionaries\n",
    "\n",
    "# filings_loughran_neg_score: { (ticker, date) : (loughran_neg_score,\n",
    "# harvard_neg_score) }\n",
    "\n",
    "filings_neg_score = {}\n",
    "\n",
    "for filing in sample_filings_corpus:\n",
    "    list_of_filing_tokens = sample_filings_corpus[filing]\n",
    "    loughran_neg_score = get_sentiment_score(list_of_filing_tokens, \"Loughran\")\n",
    "    harvard_neg_score = get_sentiment_score(list_of_filing_tokens, \"Harvard\")\n",
    "    filings_neg_score[filing] = (loughran_neg_score, harvard_neg_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CRSP data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/kfc4gh5d1jn6zkrmv51vvbyw0000gn/T/ipykernel_18267/377214008.py:5: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  crsp_df = pd.read_csv(CRSP_DATA_DIR + \"crsp_2020_2021.csv\", parse_dates=['date'])\n"
     ]
    }
   ],
   "source": [
    "# loading financial data from CRSP database\n",
    "# the data has been preloaded from CRSP in a form of csv file\n",
    "\n",
    "print(\"Loading CRSP data...\")\n",
    "sample_crsp_data = get_crsp_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating excess return for all companies in our sample...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/kfc4gh5d1jn6zkrmv51vvbyw0000gn/T/ipykernel_18267/135129160.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  excess_return_df =  excess_return_df[crsp_dataframe['date'] == date]\n"
     ]
    }
   ],
   "source": [
    "# calculating excess return for all companies in our sample\n",
    "\n",
    "print(\"Calculating excess return for all companies in our sample...\")\n",
    "\n",
    "# filings_excess_return : {(ticker, date) : excess_return}\n",
    "filings_excess_return = {}\n",
    "\n",
    "for filing in sample_filings_corpus:\n",
    "    filing_ticker = filing[0]\n",
    "    filing_date = filing[1]\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    datetime_date = datetime.strftime(filing_date, DATE_FORMAT)\n",
    "    filing_permno = get_permno_from_ticker(sp500_sample, filing_ticker)\n",
    "    filing_excess_return = get_excess_return(sample_crsp_data, filing_permno,\n",
    "                                             datetime_date)\n",
    "    filings_excess_return[filing] = filing_excess_return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Performing proportion of negative words analysis on the sp500 sample </h2>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Collecting negativity score for filings in our dataset and their \"\n",
    "      \"corresponding excess returns\")\n",
    "\n",
    "filings_negativity_score_prop = []\n",
    "sample_filings_excess_return = []\n",
    "filing_lengths = []\n",
    "\n",
    "for filing in sample_filings_corpus:\n",
    "    filing_lengths.append(len(sample_filings_corpus[filing]))\n",
    "    filings_negativity_score_prop.append(filings_neg_score[filing])\n",
    "    sample_filings_excess_return.append(filings_excess_return[filing])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Separating proportion of negative words results for Loughran and \"\n",
    "      \"Harvard dictionaries\")\n",
    "\n",
    "filings_negativity_score_prop_lr = []\n",
    "filings_negativity_score_prop_hv = []\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for sentiment_score in filings_negativity_score_prop:\n",
    "    filings_negativity_score_prop_lr.append(sentiment_score[0]['Negative'] /\n",
    "                                            filing_lengths[idx])\n",
    "    filings_negativity_score_prop_hv.append(sentiment_score[1]['Negative'] /\n",
    "                                            filing_lengths[idx])\n",
    "    idx += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Displaying the final result for negativity proportion analysis for \"\n",
    "      \"Loughran dictionary\")\n",
    "\n",
    "plot_negativity_vs_median_excess_return(sample_filings_excess_return,\n",
    "                                        filings_negativity_score_prop_lr, \"Fin_Neg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Displaying the final result for negativity proportion analysis for \"\n",
    "      \"Harvard dictionary\")\n",
    "\n",
    "plot_negativity_vs_median_excess_return(sample_filings_excess_return,\n",
    "                                        filings_negativity_score_prop_hv, \"Harvard\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Performing TFIDF-based analysis on the sp500 sample<h2>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing sentiment dictionary based on the Loughran financial-term dictionary\n"
     ]
    },
    {
     "data": {
      "text/plain": "            Word  Negative\n9        ABANDON      2009\n10     ABANDONED      2009\n11    ABANDONING      2009\n12   ABANDONMENT      2009\n13  ABANDONMENTS      2009",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9</th>\n      <td>ABANDON</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>ABANDONED</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ABANDONING</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>ABANDONMENT</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ABANDONMENTS</td>\n      <td>2009</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing a sentiment dictionary based on the Loughran financial-term\n",
    "# dictionary\n",
    "# the dictionary itself has been preloaded as a csv file\n",
    "print(\"Constructing sentiment dictionary based on the Loughran financial-term dictionary\")\n",
    "\n",
    "loughran_sentiment_dict = construct_sentiment_dict()\n",
    "loughran_sentiment_dict.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing sentiment dictionary based on the Harvard general sentiment dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/kfc4gh5d1jn6zkrmv51vvbyw0000gn/T/ipykernel_18267/1405762866.py:15: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(DICTIONARY_DIR + \"Harvard_Negative_Word_List.txt\",\n"
     ]
    },
    {
     "data": {
      "text/plain": "        ABANDON\n0     ABANDONED\n1    ABANDONING\n2   ABANDONMENT\n3  ABANDONMENTS\n4      ABANDONS",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ABANDON</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ABANDONED</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ABANDONING</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ABANDONMENT</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ABANDONMENTS</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ABANDONS</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Constructing sentiment dictionary based on the Harvard general sentiment dictionary\")\n",
    "\n",
    "harvard_sentiment_dict = construct_sentiment_dict(\"Harvard\")\n",
    "harvard_sentiment_dict.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating TFIDF scores for all tokens in all filings\n",
      "Loading serialized version of tfidf_score_map\n",
      "Serialized version of tfidf_score_map was not found, creating the map from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennisfenchenko/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# calculating TFIDF score for every filing within the corpus\n",
    "load_serialized_version = True\n",
    "SERIALIZATION_DIR = \"serialized/\"\n",
    "\n",
    "print(\"Calculating TFIDF scores for all tokens in all filings\")\n",
    "\n",
    "# retrieving filing body from each filing within the corpus\n",
    "filings_corpus = []\n",
    "\n",
    "for filing_body in sample_filings_corpus.values():\n",
    "    filings_corpus.extend(filing_body)\n",
    "\n",
    "# calculating TFIDF scores for each of the terms within the corpus\n",
    "if load_serialized_version:\n",
    "    print(\"Loading serialized version of tfidf_score_map\")\n",
    "    try:\n",
    "        tfidf_score_map = pickle.load(open(SERIALIZATION_DIR + \"tfidf_score_map.pickle\", \"rb\"))\n",
    "    except (OSError, IOError) as e:\n",
    "        print(\"Serialized version of tfidf_score_map was not found, creating \"\n",
    "              \"the map from scratch...\")\n",
    "        tfidf_score_map = calc_tfidf_score(filings_corpus)\n",
    "        pickle.dump(tfidf_score_map, open(SERIALIZATION_DIR +\n",
    "                                          \"tfidf_score_map.pickle\", \"wb\"))\n",
    "else:\n",
    "    tfidf_score_map = calc_tfidf_score(filings_corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function calculates negativity score of the document based on the\n",
    "# tfidf score\n",
    "def calculate_tfids_neg_score(document, doc_idx, tfidf_corpus_scores, \\\n",
    " dictionary):\n",
    "    curr_neg_score = 0\n",
    "    doc_term_dict = construct_term_dict(document)\n",
    "    for term in doc_term_dict:\n",
    "        if term.upper() in dictionary['Word'].values:\n",
    "            curr_neg_score += tfidf_corpus_scores.iloc[[doc_idx]][term]\n",
    "    return curr_neg_score / len(document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculating tfidf negativity scores for all the documents in our collection\n",
    "\n",
    "print(\"Collecting tfidf negativity score for filings in our dataset and their \"\n",
    "      \"corresponding excess returns using Loughran and Harvard dictionaries.\")\n",
    "\n",
    "filings_tfidf_neg_score = {}\n",
    "\n",
    "doc_idx = 0\n",
    "\n",
    "for filing in sample_filings_corpus:\n",
    "\n",
    "        filing_tfidf_score_lr = calculate_tfids_neg_score\\\n",
    "(sample_filings_corpus[filing], doc_idx, tfidf_score_map, loughran_sentiment_dict)\n",
    "\n",
    "        filing_tfidf_score_hv = calculate_tfids_neg_score\\\n",
    "(sample_filings_corpus[filing], doc_idx, tfidf_score_map, harvard_sentiment_dict)\n",
    "\n",
    "        filings_tfidf_neg_score[filing] = (filing_tfidf_score_lr, filing_tfidf_score_hv)\n",
    "\n",
    "        doc_idx += 1\n",
    "\n",
    "\n",
    "filings_negativity_score_tfidf_lr = []\n",
    "filings_negativity_score_tfidf_hv = []\n",
    "sample_filings_excess_return = []\n",
    "\n",
    "for filing in sample_filings_corpus:\n",
    "\n",
    "    filings_negativity_score_tfidf_lr.append\\\n",
    "(filings_tfidf_neg_score[filing][0])\n",
    "\n",
    "    filings_negativity_score_tfidf_hv.append(filings_tfidf_neg_score[filing][1])\n",
    "\n",
    "    sample_filings_excess_return.append(filings_excess_return[filing])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Displaying the final result for TFIDF analysis using Loughran dictionary\")\n",
    "\n",
    "plot_negativity_vs_median_excess_return(filings_negativity_score_tfidf_lr, sample_filings_excess_return)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Displaying the final result for TFIDF analysis using Harvard dictionary\")\n",
    "\n",
    "plot_negativity_vs_median_excess_return(filings_negativity_score_tfidf_hv, sample_filings_excess_return)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
