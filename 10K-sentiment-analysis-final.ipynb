{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "TODO:\n",
    "* add plotting functionality\n",
    "* pickle data that doesn't change frequently (like dictionaries)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "\n",
    "# import project packages\n",
    "import wrds\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sec_edgar_downloader import Downloader\n",
    "import time\n",
    "import calendar\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pysentiment2 as ps\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# this function retrieves all sp500 records from the WRDS database\n",
    "# the records are returned in Pandas dataframe format\n",
    "def get_sp500_records():\n",
    "    WRDS_USERNAME = 'denisfench'\n",
    "    # establish connection to the WRDS database\n",
    "    conn = wrds.Connection(wrds_username=WRDS_USERNAME)\n",
    "\n",
    "    ### Get S&P500 Index Membership from CRSP\n",
    "    sp500 = conn.raw_sql(\"\"\"\n",
    "                            select a.*, b.date, b.ret\n",
    "                            from crsp.msp500list as a,\n",
    "                            crsp.msf as b\n",
    "                            where a.permno=b.permno\n",
    "                            and b.date >= a.start and b.date<= a.ending\n",
    "                            and b.date>='01/01/2000'\n",
    "                            order by date;\n",
    "                            \"\"\", date_cols=['start', 'ending', 'date'])\n",
    "\n",
    "    ### Add Other Company Identifiers from CRSP.MSENAMES, such as TICKER, SHRCD, EXCHCD\n",
    "    mse = conn.raw_sql(\"\"\"\n",
    "                            select comnam, ncusip, namedt, nameendt,\n",
    "                            permno, shrcd, exchcd, hsiccd, ticker\n",
    "                            from crsp.msenames\n",
    "                            \"\"\", date_cols=['namedt', 'nameendt'])\n",
    "\n",
    "    # if nameendt is missing then set to today date\n",
    "    mse['nameendt']=mse['nameendt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "    # Merge with SP500 data\n",
    "    sp500_full = pd.merge(sp500, mse, how = 'left', on = 'permno')\n",
    "\n",
    "    # Impose the date range restrictions\n",
    "    sp500_full = sp500_full.loc[(sp500_full.date>=sp500_full.namedt) \\\n",
    "                                & (sp500_full.date<=sp500_full.nameendt)]\n",
    "\n",
    "    ### Add Other Company Identifiers from CRSP.MSENAMES\n",
    "    mse = conn.raw_sql(\"\"\"\n",
    "                            select comnam, ncusip, namedt, nameendt,\n",
    "                            permno, shrcd, exchcd, hsiccd, ticker\n",
    "                            from crsp.msenames\n",
    "                            \"\"\", date_cols=['namedt', 'nameendt'])\n",
    "\n",
    "    # if nameendt is missing then set to today date\n",
    "    mse['nameendt']=mse['nameendt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "    # Merge with SP500 data\n",
    "    sp500_full = pd.merge(sp500, mse, how = 'left', on = 'permno')\n",
    "\n",
    "    # Impose the date range restrictions\n",
    "    sp500_full = sp500_full.loc[(sp500_full.date>=sp500_full.namedt) \\\n",
    "                                & (sp500_full.date<=sp500_full.nameendt)]\n",
    "\n",
    "    ### Add Compustat Identifiers\n",
    "    ccm=conn.raw_sql(\"\"\"\n",
    "                      select gvkey, liid as iid, lpermno as permno,\n",
    "                      linktype, linkprim, linkdt, linkenddt\n",
    "                      from crsp.ccmxpf_linktable\n",
    "                      where substr(linktype,1,1)='L'\n",
    "                      and (linkprim ='C' or linkprim='P')\n",
    "                      \"\"\", date_cols=['linkdt', 'linkenddt'])\n",
    "\n",
    "    # if linkenddt is missing then set to today date\n",
    "    ccm['linkenddt']=ccm['linkenddt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "    # Merge the CCM data with S&P500 data\n",
    "    # First just link by matching PERMNO\n",
    "    sp500ccm = pd.merge(sp500_full, ccm, how='left', on=['permno'])\n",
    "\n",
    "    # Then set link date bounds\n",
    "    sp500ccm = sp500ccm.loc[(sp500ccm['date']>=sp500ccm['linkdt'])\\\n",
    "                            &(sp500ccm['date']<=sp500ccm['linkenddt'])]\n",
    "\n",
    "    # Rearrange columns for final output\n",
    "    sp500ccm = sp500ccm.drop(columns=['namedt', 'nameendt', 'linktype', \\\n",
    "                                      'linkprim', 'linkdt', 'linkenddt'])\n",
    "    sp500ccm = sp500ccm[['date', 'permno', 'comnam', 'ncusip',\\\n",
    "                         'shrcd', 'exchcd', 'hsiccd', 'ticker', \\\n",
    "                         'gvkey', 'iid', 'start', 'ending', 'ret']]\n",
    "\n",
    "    ### Add CIKs and Link with SEC Index Files using CIK\n",
    "    names = conn.raw_sql(\"\"\" select gvkey, cik, sic, naics, gind, gsubind from comp.names \"\"\")\n",
    "\n",
    "    # Merge sp500 constituents table with names table\n",
    "    sp500 = pd.merge(sp500ccm, names, on='gvkey',  how='left')\n",
    "    return sp500"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# this function retrieves a subsample of the sp500 records corresponding to\n",
    "# the given dates (YYYY-MM-DD)\n",
    "# EX: get_sp500_records_by_date(sp500, '2020-01-01', '2021-12-31')\n",
    "\n",
    "def get_sp500_records_by_date(sp500_record, start_date, end_date):\n",
    "    new_sp500_sample = sp500_record.loc[start_date <= sp500_record\n",
    "        .date][['date',\n",
    "                                                                    'permno',\n",
    "                                                      'comnam',\n",
    "                                               'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "\n",
    "    new_sp500_sample = new_sp500_sample.loc[sp500_record.date <=\n",
    "                                      end_date][['date',\n",
    "                                                                        'permno',\n",
    "                                                          'comnam',\n",
    "                                                   'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "\n",
    "    return new_sp500_sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# increment the current date by 1 month\n",
    "def get_next_month(date, DATE_FORMAT=\"%Y-%m-%d\", datetime_format=False):\n",
    "    datetime_date = datetime.strptime(date, DATE_FORMAT)\n",
    "    new_date = datetime_date + relativedelta(months=1)\n",
    "    if datetime_format:\n",
    "        return new_date\n",
    "    else:\n",
    "        return new_date.strftime(DATE_FORMAT)\n",
    "\n",
    "\n",
    "# increment the current date by 1 day\n",
    "def get_next_day(date, datetime_format=False):\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    datetime_date = datetime.strptime(date, DATE_FORMAT)\n",
    "    new_date = datetime_date + relativedelta(days=1)\n",
    "    if datetime_format:\n",
    "        return new_date\n",
    "    else:\n",
    "        return new_date.strftime(DATE_FORMAT)\n",
    "\n",
    "\n",
    "# get the last day of the current month\n",
    "def get_end_month_date(curr_date, datetime_format=False):\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    datetime_date = datetime.strptime(curr_date, DATE_FORMAT)\n",
    "    current_date = curr_date\n",
    "    month_end_date = current_date[:8] + str(calendar.monthrange(datetime_date.year,\n",
    "                                                datetime_date\n",
    "                                      .month)[1])\n",
    "    month_end_date = datetime.strptime(month_end_date, DATE_FORMAT)\n",
    "    if datetime_format:\n",
    "        return month_end_date\n",
    "    else:\n",
    "        return month_end_date.strftime(DATE_FORMAT)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# this function retrieves all 10-K and 10-Q fillings for a given company for\n",
    "# a given month\n",
    "def get_filing_by_month(ciks, date, dir_name):\n",
    "    num_filings = 0\n",
    "    dl = Downloader(dir_name)\n",
    "    for cik in ciks:\n",
    "        num_filings += dl.get(\"10-Q\", cik, after=date, before=get_next_month\n",
    "        (date))\n",
    "        num_filings += dl.get(\"10-K\", cik, after=date, before=get_next_month\n",
    "        (date))\n",
    "    return num_filings\n",
    "\n",
    "# this function retrieves all 10-K and 10-Q fillings for a given company for\n",
    "# a given day\n",
    "def get_filing_by_day(ciks, date, dir_name):\n",
    "    num_filings = 0\n",
    "    dl = Downloader(dir_name)\n",
    "    for cik in ciks:\n",
    "        num_filings += dl.get(\"10-Q\", cik, after=date, before=get_next_day\n",
    "        (date))\n",
    "        num_filings += dl.get(\"10-K\", cik, after=date, before=get_next_day\n",
    "        (date))\n",
    "    return num_filings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# this function retrieves all S&P 500 10-Q or 10-K filings for a given month,\n",
    "# if such a filing exists for a given company and writes them to a given directory\n",
    "# the date should be passed in the following format: YYYY-MM-DD\n",
    "# the company lookup is based in the cik number by default\n",
    "def get_all_sp_filings_by_month(sp500_collection, date, dir_name,\n",
    "                                identifier='cik'):\n",
    "    sp500_ids = []\n",
    "    num_filings = 0\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    current_date = date\n",
    "    month_end_date = get_end_month_date(current_date, datetime_format=True)\n",
    "\n",
    "    while datetime.strptime(current_date, DATE_FORMAT) <= month_end_date:\n",
    "        sp500_companies = sp500_collection.loc[sp500_collection.date ==\n",
    "                                               current_date][['date',\n",
    "                                                                        'permno',\n",
    "                                                          'comnam',\n",
    "                                                   'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "\n",
    "        sp500_ids.extend(sp500_companies[identifier].tolist())\n",
    "        current_date = get_next_day(current_date)\n",
    "\n",
    "    num_filings += get_filing_by_month(sp500_ids,\n",
    "                                       current_date,\n",
    "                                       dir_name)\n",
    "\n",
    "    return num_filings\n",
    "\n",
    "# this function retrieves all S&P 500 10-Q or 10-K filings for a given day,\n",
    "# if such a filing exists for a given company and writes them to a given directory\n",
    "# the date should be passed in the following format: YYYY-MM-DD\n",
    "def get_all_sp_filings_by_day(sp500_collection, date, dir_name, identifier='cik'):\n",
    "    num_filings = 0\n",
    "    sp500_companies = sp500_collection.loc[sp500_collection.date == date][['date',\n",
    "                                                                    'permno',\n",
    "                                                      'comnam',\n",
    "                                               'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "    sp500_tickers = sp500_companies[identifier]\n",
    "\n",
    "    for ticker in sp500_tickers:\n",
    "        num_filings += get_filing_by_day(ticker, date, dir_name)\n",
    "\n",
    "    return num_filings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# this function retrieves all S&P 500 10-Q and 10-K filings for a given period\n",
    "def get_sp500_filings(sp500_collection, filings_start_date, filings_end_date,\n",
    "                      dir_name):\n",
    "\n",
    "    print(\"Retrieving all the 10-Q and 10-K filings starting from \",\n",
    "           filings_start_date, \" until \", filings_end_date)\n",
    "\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    current_date = filings_start_date\n",
    "    num_filings = 0\n",
    "    while datetime.strptime(current_date, DATE_FORMAT) <= datetime.strptime\\\n",
    "                (filings_end_date, DATE_FORMAT):\n",
    "        num_filings += get_all_sp_filings_by_month(sp500_collection, current_date,\n",
    "                                                   dir_name)\n",
    "        current_date = get_next_month(current_date)\n",
    "\n",
    "        print(\"Successfully retrieved \" + str(num_filings) + \" for \" + str\n",
    "        (current_date))\n",
    "\n",
    "    return num_filings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# this function retrieves all tickers or ciks for all companies from a given\n",
    "# directory\n",
    "def get_all_filing_ids(dir_name):\n",
    "    ids = os.listdir(dir_name + \"/sec-edgar-filings\")\n",
    "    return ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# this function extracts the filing date, given the filing date line from the\n",
    "# filing document\n",
    "def extract_filing_date(filed_as_of_date_line):\n",
    "    DATE_FORMAT = \"%Y%m%d\"\n",
    "    date = \"\"\n",
    "    for ch in filed_as_of_date_line:\n",
    "        if ch.isdigit():\n",
    "            date += ch\n",
    "    return datetime.strptime(date, DATE_FORMAT).date()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "FILING_DETAILS = \"filing-details.html\"\n",
    "FULL_SUBMISSION = \"full-submission.txt\"\n",
    "FILED_AS_OF_DATE_IDX = 7\n",
    "\n",
    "# this function will search for a filing in a given directory based on the\n",
    "# given ticker or cik\n",
    "# it will return a dictionary with a ticker and the date of filing as the key\n",
    "# and filing text as the value\n",
    "# currently the function doesn't indicate whether the filing is a 10-Q or 10-K\n",
    "def get_filings_from_id(comp_id, dir_name, sec_filings):\n",
    "    for root, dirs, files in os.walk(dir_name + \"/sec-edgar-filings/\" + comp_id):\n",
    "        for f in files:\n",
    "            if FILING_DETAILS in f:\n",
    "                with open(root + \"/\" + f) as fp:\n",
    "                    soup = BeautifulSoup(fp, \"html.parser\")\n",
    "                    line_idx = 0\n",
    "                    with open(root + \"/\" + FULL_SUBMISSION) as fs_fp:\n",
    "                        while line_idx < FILED_AS_OF_DATE_IDX:\n",
    "                            line_idx += 1\n",
    "                            fs_fp.readline()\n",
    "                        date = extract_filing_date(fs_fp.readline())\n",
    "                    sec_filings[(comp_id, date)] = soup.get_text()\n",
    "    return sec_filings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# get filing bosy for all filings in the list containing filing ids\n",
    "def get_filings_from_comp_ids(ids, dir_name):\n",
    "    sec_filings = {}\n",
    "    for comp_id in ids:\n",
    "        sec_filings[comp_id] = get_filings_from_id(comp_id, dir_name, sec_filings)\n",
    "    return sec_filings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# this function sets up SSL context for downloading packages from nltk library\n",
    "def create_ssl_context():\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# a function that tokenizes 10-K or 10-Q corpus that\n",
    "def tokenize_filing(filing_corpus):\n",
    "    filing_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return filing_tokenizer.tokenize(filing_corpus)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# a function to remove the stop words from the filings corpus\n",
    "def filter_out_stopwords(tokenized_filing_corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filing_corpus_filtered = [word for word in tokenized_filing_corpus if not \\\n",
    "        word.lower() in stop_words]\n",
    "    return filing_corpus_filtered\n",
    "\n",
    "\n",
    "# a function that filters out numbers from a filing corpus\n",
    "def filter_out_numbers(filing_corpus):\n",
    "    return [token for token in filing_corpus if not (token.isdigit()\n",
    "                                         or token[0] == '-' and token[1:].isdigit())]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# this function filters out both the stop words and the numbers\n",
    "def clean_filing_corpus(tokenized_filing_corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filing_corpus_filtered = [word for word in tokenized_filing_corpus if not \\\n",
    "        word.lower() in stop_words and word if not (word.isdigit()\n",
    "                                         or word[0] == '-' and word[1:].isdigit())]\n",
    "    return filing_corpus_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on 10-Q and 10-K filing documents\n",
    "\n",
    "# this function calculates the sentiment score of the text based on a\n",
    "# sentiment dictionary (Loughran or Harvard), passed in as a parameter\n",
    "def get_sentiment_score(text, dictionary):\n",
    "\n",
    "    # get the Harvard general sentiment dictionary\n",
    "    hiv4 = ps.HIV4()\n",
    "\n",
    "    # get the Loughran and McDonald dictionary\n",
    "    lm = ps.LM()\n",
    "\n",
    "    if dictionary == \"Harvard\":\n",
    "        return hiv4.get_sentiment_score(text)\n",
    "\n",
    "    if dictionary == \"Loughran\":\n",
    "        return lm.get_sentiment_score(text)\n",
    "\n",
    "    raise ValueError(\"The dictionary could not be found.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# this function retrieves Value-Weighted Return (including distributions) and\n",
    "# Holding Period Return from the CRSP database\n",
    "def get_crsp_data():\n",
    "    CRSP_DATA_DIR = \"CRSP-data/\"\n",
    "    crsp_df = pd.read_csv(CRSP_DATA_DIR + \"crsp.csv\", parse_dates=['date'])\n",
    "    return crsp_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "PERMNO_IDX = 2\n",
    "\n",
    "def get_permno_from_ticker(sp500, ticker):\n",
    "    return int(sp500.loc[sp500['ticker'] == ticker, 'permno'].iloc[PERMNO_IDX])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# this function calculates excess return using Value-Weighted Return (including distributions) and\n",
    "# Holding Period Return from the CRSP database\n",
    "def get_excess_return(crsp_dataframe, permno, date):\n",
    "    excess_return_df = crsp_dataframe[crsp_dataframe['PERMNO'] == permno]\n",
    "    excess_return_df =  excess_return_df[crsp_dataframe['date'] == date]\n",
    "    excess_return = float(excess_return_df['vwretd']) - \\\n",
    "                    float(excess_return_df['RET'])\n",
    "    return excess_return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "DICTIONARY_DIR = \"dictionaries/\"\n",
    "\n",
    "# this function constructs a sentiment dictionary based on the Loughran\n",
    "# McDonald and Harvard dictionaries\n",
    "# return: {Term: [str], Sentiment: [int], Term_Count: [int]}\n",
    "# where Term count is the number of times the term occurs within the corpus\n",
    "def construct_sentiment_dict(base_dict=\"Loughran\"):\n",
    "    if base_dict == \"Loughran\":\n",
    "        df = pd.read_csv(DICTIONARY_DIR +\n",
    "                         \"Loughran-McDonald_MasterDictionary_1993-2021.csv\",\n",
    "                         header=0, usecols=['Word', 'Negative', 'Positive'])\n",
    "    else:\n",
    "        df = pd.read_csv(DICTIONARY_DIR + \"Harvard.csv\", header=0, usecols=['Word', 'Negative', 'Positive'])\n",
    "\n",
    "    df = df[(df['Negative'] != 0) | (df['Positive'] != 0)]\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "            Word  Negative  Positive\n9        ABANDON      2009         0\n10     ABANDONED      2009         0\n11    ABANDONING      2009         0\n12   ABANDONMENT      2009         0\n13  ABANDONMENTS      2009         0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Negative</th>\n      <th>Positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9</th>\n      <td>ABANDON</td>\n      <td>2009</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>ABANDONED</td>\n      <td>2009</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ABANDONING</td>\n      <td>2009</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>ABANDONMENT</td>\n      <td>2009</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ABANDONMENTS</td>\n      <td>2009</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing Loughran sentiment dictionary\n",
    "lr_dict = construct_sentiment_dict()\n",
    "lr_dict.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# This function constructs a term dictionary based on a given document: tokenized filing list\n",
    "# return: {Term: [str], Term_Count: [int]}\n",
    "# where Term count is the number of times the term occurs within the document\n",
    "def construct_term_dict(document):\n",
    "    doc_dict = {}\n",
    "    for token in document:\n",
    "        if token in doc_dict:\n",
    "            doc_dict[token] += 1\n",
    "        else:\n",
    "            doc_dict[token] = 1\n",
    "    return doc_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# This function calculates a document negativity score based on the\n",
    "# proportion of negative words in the document\n",
    "# the negativity of the term is determined based on a given dictionary\n",
    "# (Loughran or Harvard)\n",
    "def compute_negativity_score_based_on_proportion(text,\n",
    "                                                 sentiment_dict=\"Loughran\"):\n",
    "    sentiment_score = get_sentiment_score(text, sentiment_dict)\n",
    "    num_negative = sentiment_score['Negative']\n",
    "    return num_negative / len(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# *** TO BE REMOVED ***\n",
    "\n",
    "# This function calculates a document negativity score based on the\n",
    "# TFIDF score for the document\n",
    "# TFIDF:\n",
    "# TF (Term Frequency) (t, d) = (# Occurrences of the word in a document / # Total words in a document )\n",
    "# IDF (Inverse Document Frequency) (t, D) = log ( (N (number of documents in a collection) / (# Documents containing the term t))\n",
    "# TFIDF (t, d, D) = TF (t, d) * IDF (t, D)\n",
    "# the function takes document dictionary, containing terms and their\n",
    "# frequencies for a given document; the sentiment dictionary, either Loughran\n",
    "# or Harvard, containing token sentiment rating as well as the document term\n",
    "# count for the entire corpus; the number of documents in the collection and\n",
    "# the number of tokens within a given document\n",
    "def get_tfidf_negativity_score(doc_dict, sentiment_dict, doc_term_freq,\n",
    "                               collection_size):\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# This function calculates TFIDF score for the filings based on the sentiment\n",
    "# dictionary vocabulary using SciKit Learn\n",
    "def calc_tfidf_score(sentiment_dict, corpus):\n",
    "    vectorizer = TfidfVectorizer(vocabulary=sentiment_dict.keys(),\n",
    "                                 stop_words=\"english\")\n",
    "    vectors = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    dense_list = dense.tolist()\n",
    "    tfidf_df = pd.DataFrame(dense_list, columns=feature_names)\n",
    "    return tfidf_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# this function loads all the filings from a given directory, which\n",
    "# constitute an entire corpus of documents\n",
    "def load_filings_corpus(corpus_root_dir):\n",
    "    FILING_DETAILS = \"filing-details.html\"\n",
    "    sec_filings_corpus = []\n",
    "    for root, dirs, files in os.walk(corpus_root_dir + \"/sec-edgar-filings/\"):\n",
    "        for f in files:\n",
    "            if FILING_DETAILS in f:\n",
    "                with open(root + \"/\" + f) as file_ptr:\n",
    "                    bs = BeautifulSoup(file_ptr, \"html.parser\")\n",
    "                    sec_filings_corpus.append(bs.get_text())\n",
    "    return sec_filings_corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function plots negativity score (quantile) vs median excess return for a\n",
    "# sample\n",
    "# of companies\n",
    "\n",
    "def plot_negativity_vs_median_excess_return\\\n",
    "(quantile, median_filing_period_excess_return):\n",
    "    plt.xlabel('Quintile (based on proportion of negative words)')\n",
    "    plt.ylabel('Median Filling Period Excess Return')\n",
    "    plt.axis().legend(['Fin-Neg'])\n",
    "    plt.plot(quantile, median_filing_period_excess_return)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>SENTIMENT ANALYSIS OF 10-Qs and 10-Ks for S&P 500 companies for 2020,\n",
    "2021<h1>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "create_ssl_context()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# getting all S&P 500 records from the WRDS database\n",
    "\n",
    "sp500_all = get_sp500_records()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "             date   permno               comnam    ncusip   gvkey iid  \\\n120131 2020-01-31  34746.0  FIFTH THIRD BANCORP  31677310  004640  01   \n120132 2020-01-31  75591.0            IDEX CORP  45167R10  015267  01   \n120133 2020-01-31  14541.0     CHEVRON CORP NEW  16676410  002991  01   \n120134 2020-01-31  14593.0            APPLE INC  03783310  001690  01   \n120135 2020-01-31  75341.0     DUKE REALTY CORP  26441150  013510  01   \n\n               cik ticker   sic   naics  \n120131  0000035527   FITB  6020  522110  \n120132  0000832101    IEX  3561  333914  \n120133  0000093410    CVX  2911  324110  \n120134  0000320193   AAPL  3663  334220  \n120135  0000783280    DRE  6798  531120  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>permno</th>\n      <th>comnam</th>\n      <th>ncusip</th>\n      <th>gvkey</th>\n      <th>iid</th>\n      <th>cik</th>\n      <th>ticker</th>\n      <th>sic</th>\n      <th>naics</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>120131</th>\n      <td>2020-01-31</td>\n      <td>34746.0</td>\n      <td>FIFTH THIRD BANCORP</td>\n      <td>31677310</td>\n      <td>004640</td>\n      <td>01</td>\n      <td>0000035527</td>\n      <td>FITB</td>\n      <td>6020</td>\n      <td>522110</td>\n    </tr>\n    <tr>\n      <th>120132</th>\n      <td>2020-01-31</td>\n      <td>75591.0</td>\n      <td>IDEX CORP</td>\n      <td>45167R10</td>\n      <td>015267</td>\n      <td>01</td>\n      <td>0000832101</td>\n      <td>IEX</td>\n      <td>3561</td>\n      <td>333914</td>\n    </tr>\n    <tr>\n      <th>120133</th>\n      <td>2020-01-31</td>\n      <td>14541.0</td>\n      <td>CHEVRON CORP NEW</td>\n      <td>16676410</td>\n      <td>002991</td>\n      <td>01</td>\n      <td>0000093410</td>\n      <td>CVX</td>\n      <td>2911</td>\n      <td>324110</td>\n    </tr>\n    <tr>\n      <th>120134</th>\n      <td>2020-01-31</td>\n      <td>14593.0</td>\n      <td>APPLE INC</td>\n      <td>03783310</td>\n      <td>001690</td>\n      <td>01</td>\n      <td>0000320193</td>\n      <td>AAPL</td>\n      <td>3663</td>\n      <td>334220</td>\n    </tr>\n    <tr>\n      <th>120135</th>\n      <td>2020-01-31</td>\n      <td>75341.0</td>\n      <td>DUKE REALTY CORP</td>\n      <td>26441150</td>\n      <td>013510</td>\n      <td>01</td>\n      <td>0000783280</td>\n      <td>DRE</td>\n      <td>6798</td>\n      <td>531120</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieving all S&P 500 records for the two-year period (from '2020-01-01'\n",
    "# to '2021-12-31')\n",
    "\n",
    "sp500_sample = get_sp500_records_by_date(sp500_all, \"2020-01-01\", \"2021-12-31\")\n",
    "sp500_sample.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving all the 10-Q and 10-K filings starting from  2020-01-01  until  2021-12-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennisfenchenko/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [83], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m end_date \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2021-12-31\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      4\u001B[0m sp_filings_dir_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msp500_filings_sample\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 5\u001B[0m num_filings_sample \u001B[38;5;241m=\u001B[39m \u001B[43mget_sp500_filings\u001B[49m\u001B[43m(\u001B[49m\u001B[43msp500_sample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_date\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_date\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msp_filings_dir_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [24], line 13\u001B[0m, in \u001B[0;36mget_sp500_filings\u001B[0;34m(sp500_collection, filings_start_date, filings_end_date, dir_name)\u001B[0m\n\u001B[1;32m     10\u001B[0m num_filings \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m datetime\u001B[38;5;241m.\u001B[39mstrptime(current_date, DATE_FORMAT) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mstrptime\\\n\u001B[1;32m     12\u001B[0m             (filings_end_date, DATE_FORMAT):\n\u001B[0;32m---> 13\u001B[0m     num_filings \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mget_all_sp_filings_by_month\u001B[49m\u001B[43m(\u001B[49m\u001B[43msp500_collection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcurrent_date\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mdir_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     current_date \u001B[38;5;241m=\u001B[39m get_next_month(current_date)\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSuccessfully retrieved \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(num_filings) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m\n\u001B[1;32m     18\u001B[0m     (current_date))\n",
      "Cell \u001B[0;32mIn [75], line 23\u001B[0m, in \u001B[0;36mget_all_sp_filings_by_month\u001B[0;34m(sp500_collection, date, dir_name, identifier)\u001B[0m\n\u001B[1;32m     20\u001B[0m     sp500_ids\u001B[38;5;241m.\u001B[39mextend(sp500_companies[identifier]\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[1;32m     21\u001B[0m     current_date \u001B[38;5;241m=\u001B[39m get_next_day(current_date)\n\u001B[0;32m---> 23\u001B[0m num_filings \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mget_filing_by_month\u001B[49m\u001B[43m(\u001B[49m\u001B[43msp500_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mcurrent_date\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mdir_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m num_filings\n",
      "Cell \u001B[0;32mIn [82], line 7\u001B[0m, in \u001B[0;36mget_filing_by_month\u001B[0;34m(ciks, date, dir_name)\u001B[0m\n\u001B[1;32m      5\u001B[0m dl \u001B[38;5;241m=\u001B[39m Downloader(dir_name)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m cik \u001B[38;5;129;01min\u001B[39;00m ciks:\n\u001B[0;32m----> 7\u001B[0m     num_filings \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mdl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m10-Q\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcik\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mafter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbefore\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mget_next_month\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43mdate\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     num_filings \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m dl\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m10-K\u001B[39m\u001B[38;5;124m\"\u001B[39m, cik, after\u001B[38;5;241m=\u001B[39mdate, before\u001B[38;5;241m=\u001B[39mget_next_month\n\u001B[1;32m     10\u001B[0m     (date))\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m num_filings\n",
      "File \u001B[0;32m~/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/sec_edgar_downloader/Downloader.py:173\u001B[0m, in \u001B[0;36mDownloader.get\u001B[0;34m(self, filing, ticker_or_cik, amount, after, before, include_amends, download_details, query)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(query, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuery must be of type string.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 173\u001B[0m filings_to_fetch \u001B[38;5;241m=\u001B[39m \u001B[43mget_filing_urls_to_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mticker_or_cik\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamount\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[43mafter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbefore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_amends\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    183\u001B[0m download_filings(\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_folder,\n\u001B[1;32m    185\u001B[0m     ticker_or_cik,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    188\u001B[0m     download_details,\n\u001B[1;32m    189\u001B[0m )\n\u001B[1;32m    191\u001B[0m \u001B[38;5;66;03m# Get number of unique accession numbers downloaded\u001B[39;00m\n",
      "File \u001B[0;32m~/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/sec_edgar_downloader/_utils.py:160\u001B[0m, in \u001B[0;36mget_filing_urls_to_download\u001B[0;34m(filing_type, ticker_or_cik, num_filings_to_download, after_date, before_date, include_amends, query)\u001B[0m\n\u001B[1;32m    147\u001B[0m payload \u001B[38;5;241m=\u001B[39m form_request_payload(\n\u001B[1;32m    148\u001B[0m     ticker_or_cik,\n\u001B[1;32m    149\u001B[0m     [filing_type],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    153\u001B[0m     query,\n\u001B[1;32m    154\u001B[0m )\n\u001B[1;32m    155\u001B[0m headers \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUser-Agent\u001B[39m\u001B[38;5;124m\"\u001B[39m: generate_random_user_agent(),\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccept-Encoding\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgzip, deflate\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHost\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mefts.sec.gov\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    159\u001B[0m }\n\u001B[0;32m--> 160\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43mSEC_EDGAR_SEARCH_API_ENDPOINT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m resp\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    164\u001B[0m search_query_results \u001B[38;5;241m=\u001B[39m resp\u001B[38;5;241m.\u001B[39mjson()\n",
      "File \u001B[0;32m~/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/requests/sessions.py:635\u001B[0m, in \u001B[0;36mSession.post\u001B[0;34m(self, url, data, json, **kwargs)\u001B[0m\n\u001B[1;32m    624\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\u001B[38;5;28mself\u001B[39m, url, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, json\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    625\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001B[39;00m\n\u001B[1;32m    626\u001B[0m \n\u001B[1;32m    627\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 635\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPOST\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/requests/sessions.py:587\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    582\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    583\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    584\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    585\u001B[0m }\n\u001B[1;32m    586\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 587\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    589\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/requests/sessions.py:701\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    698\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    700\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    704\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m~/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/requests/adapters.py:489\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    487\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    488\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m chunked:\n\u001B[0;32m--> 489\u001B[0m         resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[43m            \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m            \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m            \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m            \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m            \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    502\u001B[0m     \u001B[38;5;66;03m# Send the request.\u001B[39;00m\n\u001B[1;32m    503\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    504\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(conn, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproxy_pool\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_proxy(conn)\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Make the request on the httplib connection object.\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    704\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    706\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    707\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    711\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    713\u001B[0m \u001B[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001B[39;00m\n\u001B[1;32m    714\u001B[0m \u001B[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001B[39;00m\n\u001B[1;32m    715\u001B[0m \u001B[38;5;66;03m# it will also try to release it and we'll have a double-release\u001B[39;00m\n\u001B[1;32m    716\u001B[0m \u001B[38;5;66;03m# mess.\u001B[39;00m\n\u001B[1;32m    717\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    444\u001B[0m             httplib_response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[1;32m    445\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    446\u001B[0m             \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[1;32m    447\u001B[0m             \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[1;32m    448\u001B[0m             \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[0;32m--> 449\u001B[0m             \u001B[43msix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_from\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    451\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[0;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[0;34m(value, from_value)\u001B[0m\n",
      "File \u001B[0;32m~/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    441\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    442\u001B[0m     \u001B[38;5;66;03m# Python 3\u001B[39;00m\n\u001B[1;32m    443\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 444\u001B[0m         httplib_response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    445\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    446\u001B[0m         \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[1;32m    447\u001B[0m         \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[1;32m    448\u001B[0m         \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[1;32m    449\u001B[0m         six\u001B[38;5;241m.\u001B[39mraise_from(e, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:1377\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1375\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1376\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1377\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1378\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[1;32m   1379\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:320\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 320\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[1;32m    322\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:281\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 281\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[1;32m    283\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py:704\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    703\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 704\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py:1241\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1237\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1238\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1239\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1240\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py:1099\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1098\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1099\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1100\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# get a sample of S&P 500 filings for a 2-year period\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2021-12-31\"\n",
    "sp_filings_dir_name = 'sp500_filings_sample'\n",
    "num_filings_sample = get_sp500_filings(sp500_sample, start_date, end_date, sp_filings_dir_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "FILINGS_SAMPLE_DIR = \"data\"\n",
    "# retrieving tickers of companies who's filings we have in our file system\n",
    "sample_ids = get_all_filing_ids(FILINGS_SAMPLE_DIR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading serialized version...\n",
      "Serialized version was not found. Retrieving filings and creating a serialized file.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "load_serialized_version = True\n",
    "SERIALIZATION_DIR = \"serialized/\"\n",
    "\n",
    "sample_filings_corpus = {}\n",
    "\n",
    "if (load_serialized_version):\n",
    "    print(\"Loading serialized version...\")\n",
    "    try:\n",
    "        sample_filings_corpus = pickle.load(open(SERIALIZATION_DIR +\n",
    "                                                 \"sample_filings_corpus\"\n",
    "                                                 \".pickle\", \"rb\"))\n",
    "    except (OSError, IOError) as e:\n",
    "        print(\"Serialized version was not found. Retrieving filings and \"\n",
    "              \"creating a serialized file.\")\n",
    "        sample_filings_corpus = get_filings_from_comp_ids(sample_ids,\n",
    "                                                        FILINGS_SAMPLE_DIR)\n",
    "        pickle.dump(sample_filings_corpus, open(SERIALIZATION_DIR +\n",
    "                                                \"sample_filings_corpus\"\n",
    "                                                \".pickle\", \"wb\"))\n",
    "        print(\"Done!\")\n",
    "\n",
    "else:\n",
    "    sample_filings_corpus = get_filings_from_comp_ids(sample_ids, FILINGS_SAMPLE_DIR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some of the filings we have in our sample:\n"
     ]
    },
    {
     "data": {
      "text/plain": "dict_keys([('VZ', datetime.date(2020, 2, 21)), 'VZ', ('AMZN', datetime.date(2020, 1, 31)), 'AMZN', ('CNP', datetime.date(2020, 2, 27)), 'CNP', ('RCL', datetime.date(2020, 2, 25)), 'RCL', ('CAT', datetime.date(2020, 2, 19)), 'CAT', ('AAPL', datetime.date(2020, 1, 29)), 'AAPL', ('KHC', datetime.date(2020, 2, 14)), 'KHC', ('AGN', datetime.date(2020, 2, 18)), 'AGN', ('ANET', datetime.date(2020, 2, 14)), 'ANET', ('CAH', datetime.date(2020, 2, 6)), 'CAH', ('FBHS', datetime.date(2020, 2, 26)), 'FBHS', ('PFE', datetime.date(2020, 2, 27)), 'PFE', ('REG', datetime.date(2020, 2, 18)), 'REG', ('APTV', datetime.date(2020, 2, 3)), 'APTV', ('AAL', datetime.date(2020, 2, 19)), 'AAL', ('CDW', datetime.date(2020, 2, 28)), 'CDW', ('MAR', datetime.date(2020, 2, 27)), 'MAR', ('VRSN', datetime.date(2020, 2, 14)), 'VRSN', ('KMI', datetime.date(2020, 2, 12)), 'KMI', ('SPGI', datetime.date(2020, 2, 10)), 'SPGI', ('ALLE', datetime.date(2020, 2, 18)), 'ALLE', ('SYK', datetime.date(2020, 2, 6)), 'SYK', ('PEP', datetime.date(2020, 2, 13)), 'PEP', ('FRT', datetime.date(2020, 2, 10)), 'FRT', ('SNPS', datetime.date(2020, 2, 21)), 'SNPS', ('PLD', datetime.date(2020, 2, 11)), 'PLD', ('MMM', datetime.date(2020, 2, 6)), 'MMM', ('EMN', datetime.date(2020, 2, 26)), 'EMN', ('AMT', datetime.date(2020, 2, 25)), 'AMT', ('ADI', datetime.date(2020, 2, 19)), 'ADI', ('MAA', datetime.date(2020, 2, 20)), 'MAA', ('DRE', datetime.date(2020, 2, 25)), 'DRE', ('FTV', datetime.date(2020, 2, 27)), 'FTV', ('MGM', datetime.date(2020, 2, 27)), 'MGM', ('VLO', datetime.date(2020, 2, 26)), 'VLO', ('EMR', datetime.date(2020, 2, 5)), 'EMR', ('SYY', datetime.date(2020, 2, 5)), 'SYY', ('GILD', datetime.date(2020, 2, 25)), 'GILD', ('SNA', datetime.date(2020, 2, 13)), 'SNA', ('MS', datetime.date(2020, 2, 27)), 'MS', ('ALXN', datetime.date(2020, 2, 4)), 'ALXN', ('CVX', datetime.date(2020, 2, 21)), 'CVX', ('NCLH', datetime.date(2020, 2, 27)), 'NCLH', ('UHS', datetime.date(2020, 2, 26)), 'UHS', ('MO', datetime.date(2020, 1, 31)), ('MO', datetime.date(2020, 2, 25)), 'MO', ('TXN', datetime.date(2020, 2, 20)), 'TXN', ('UNM', datetime.date(2020, 2, 18)), 'UNM', ('BK', datetime.date(2020, 2, 27)), 'BK', ('BLL', datetime.date(2020, 2, 19)), 'BLL', ('TTWO', datetime.date(2020, 2, 7)), 'TTWO', ('HCA', datetime.date(2020, 2, 20)), 'HCA', ('CDNS', datetime.date(2020, 2, 24)), 'CDNS', ('HII', datetime.date(2020, 2, 13)), 'HII', ('CTSH', datetime.date(2020, 2, 14)), 'CTSH', ('MCHP', datetime.date(2020, 2, 4)), 'MCHP', ('VTR', datetime.date(2020, 2, 24)), 'VTR', ('HIG', datetime.date(2020, 2, 21)), 'HIG', ('ETFC', datetime.date(2020, 2, 19)), 'ETFC', ('IPG', datetime.date(2020, 2, 21)), ('IPG', datetime.date(2020, 2, 24)), 'IPG', ('DISH', datetime.date(2020, 2, 19)), ('DISH', datetime.date(2020, 2, 25)), 'DISH', ('UNP', datetime.date(2020, 2, 7)), 'UNP', ('AMAT', datetime.date(2020, 2, 20)), 'AMAT', ('LLY', datetime.date(2020, 2, 19)), 'LLY', ('NTAP', datetime.date(2020, 2, 18)), 'NTAP', ('WAB', datetime.date(2020, 2, 24)), 'WAB', ('REGN', datetime.date(2020, 2, 7)), 'REGN', ('RSG', datetime.date(2020, 2, 14)), 'RSG', ('IR', datetime.date(2020, 2, 26)), 'IR', ('IQV', datetime.date(2020, 2, 18)), 'IQV', ('LMT', datetime.date(2020, 2, 7)), 'LMT', ('CI', datetime.date(2020, 2, 27)), 'CI', ('PPL', datetime.date(2020, 2, 14)), 'PPL', ('ANSS', datetime.date(2020, 2, 27)), 'ANSS', ('EL', datetime.date(2020, 2, 6)), ('EL', datetime.date(2020, 2, 27)), 'EL', ('PSX', datetime.date(2020, 2, 21)), 'PSX', ('JNJ', datetime.date(2020, 2, 18)), 'JNJ', ('QCOM', datetime.date(2020, 2, 5)), 'QCOM', ('LNT', datetime.date(2020, 2, 21)), 'LNT', ('BAC', datetime.date(2020, 2, 19)), 'BAC', ('IT', datetime.date(2020, 2, 19)), 'IT', ('SIVB', datetime.date(2020, 2, 28)), 'SIVB', ('DHI', datetime.date(2020, 2, 6)), 'DHI', ('IRM', datetime.date(2020, 2, 13)), 'IRM', ('DGX', datetime.date(2020, 2, 20)), 'DGX', ('EQR', datetime.date(2020, 2, 20)), 'EQR', ('ED', datetime.date(2020, 2, 20)), 'ED', ('DHR', datetime.date(2020, 2, 21)), 'DHR', ('HSIC', datetime.date(2020, 2, 20)), 'HSIC', ('ALB', datetime.date(2020, 2, 26)), 'ALB', ('NLOK', datetime.date(2020, 2, 7)), 'NLOK', ('WM', datetime.date(2020, 2, 13)), 'WM', ('COG', datetime.date(2020, 2, 25)), 'COG', ('RHI', datetime.date(2020, 2, 14)), 'RHI', ('TMO', datetime.date(2020, 2, 26)), 'TMO', ('ALL', datetime.date(2020, 2, 19)), ('ALL', datetime.date(2020, 2, 27)), ('ALL', datetime.date(2020, 2, 21)), 'ALL', ('BSX', datetime.date(2020, 2, 25)), 'BSX', ('FAST', datetime.date(2020, 2, 6)), 'FAST', ('ABBV', datetime.date(2020, 2, 21)), 'ABBV', ('AFL', datetime.date(2020, 2, 21)), 'AFL', ('RMD', datetime.date(2020, 1, 31)), 'RMD', ('TWTR', datetime.date(2020, 2, 19)), 'TWTR', ('EFX', datetime.date(2020, 2, 20)), 'EFX', ('LYB', datetime.date(2020, 2, 20)), 'LYB', ('PNR', datetime.date(2020, 2, 25)), 'PNR', ('KEY', datetime.date(2020, 2, 5)), ('KEY', datetime.date(2020, 2, 26)), 'KEY', ('ATVI', datetime.date(2020, 2, 27)), 'ATVI', ('CHRW', datetime.date(2020, 2, 19)), 'CHRW', ('COF', datetime.date(2020, 2, 20)), 'COF', ('PRGO', datetime.date(2020, 2, 27)), 'PRGO', ('VMC', datetime.date(2020, 2, 26)), 'VMC', ('HSY', datetime.date(2020, 2, 20)), 'HSY', ('T', datetime.date(2020, 2, 13)), ('T', datetime.date(2020, 2, 20)), ('T', datetime.date(2020, 2, 6)), 'T', ('CHTR', datetime.date(2020, 1, 31)), 'CHTR', ('TAP', datetime.date(2020, 2, 12)), 'TAP', ('MCK', datetime.date(2020, 2, 4)), 'MCK', ('AIG', datetime.date(2020, 2, 21)), 'AIG', ('EBAY', datetime.date(2020, 1, 31)), 'EBAY', ('F', datetime.date(2020, 2, 4)), ('F', datetime.date(2020, 2, 5)), 'F', ('NTRS', datetime.date(2020, 2, 25)), 'NTRS', ('O', datetime.date(2020, 2, 28)), ('O', datetime.date(2020, 2, 21)), ('O', datetime.date(2020, 2, 24)), 'O', ('CFG', datetime.date(2020, 2, 24)), 'CFG', ('STE', datetime.date(2020, 2, 10)), 'STE', ('PYPL', datetime.date(2020, 2, 6)), 'PYPL', ('IBM', datetime.date(2020, 2, 28)), ('IBM', datetime.date(2020, 2, 25)), 'IBM', ('AMD', datetime.date(2020, 2, 4)), 'AMD', ('BWA', datetime.date(2020, 2, 13)), 'BWA', ('MHK', datetime.date(2020, 2, 28)), 'MHK', ('PG', datetime.date(2020, 2, 25)), ('PG', datetime.date(2020, 2, 21)), ('PG', datetime.date(2020, 2, 20)), ('PG', datetime.date(2020, 2, 28)), ('PG', datetime.date(2020, 2, 27)), ('PG', datetime.date(2020, 2, 18)), ('PG', datetime.date(2020, 2, 19)), ('PG', datetime.date(2020, 2, 14)), 'PG', ('INCY', datetime.date(2020, 2, 13)), 'INCY', ('XOM', datetime.date(2020, 2, 26)), 'XOM', ('USB', datetime.date(2020, 2, 20)), 'USB', ('SO', datetime.date(2020, 2, 20)), 'SO', ('LRCX', datetime.date(2020, 2, 4)), 'LRCX', ('FOXA', datetime.date(2020, 2, 5)), 'FOXA', ('DTE', datetime.date(2020, 2, 5)), 'DTE', ('BRK', datetime.date(2020, 2, 24)), 'BRK', ('ROP', datetime.date(2020, 2, 28)), 'ROP', ('AME', datetime.date(2020, 2, 20)), 'AME', ('RJF', datetime.date(2020, 2, 7)), 'RJF', ('KMB', datetime.date(2020, 2, 13)), 'KMB', ('SPG', datetime.date(2020, 2, 21)), 'SPG', ('PEAK', datetime.date(2020, 2, 13)), 'PEAK', ('YUM', datetime.date(2020, 2, 27)), ('YUM', datetime.date(2020, 2, 20)), 'YUM', ('FTI', datetime.date(2020, 2, 25)), 'FTI', ('PCAR', datetime.date(2020, 2, 19)), 'PCAR', ('MDT', datetime.date(2020, 2, 28)), 'MDT', ('SYF', datetime.date(2020, 2, 13)), 'SYF', ('ABT', datetime.date(2020, 2, 21)), 'ABT', ('HOG', datetime.date(2020, 2, 19)), 'HOG', ('NOW', datetime.date(2020, 2, 19)), ('NOW', datetime.date(2020, 2, 20)), 'NOW', ('CSX', datetime.date(2020, 2, 12)), 'CSX', ('MPC', datetime.date(2020, 2, 28)), 'MPC', ('FCX', datetime.date(2020, 2, 14)), 'FCX', ('DOW', datetime.date(2020, 2, 7)), 'DOW', ('UDR', datetime.date(2020, 2, 18)), 'UDR', ('TRV', datetime.date(2020, 2, 13)), 'TRV', ('BA', datetime.date(2020, 1, 31)), 'BA', ('FTNT', datetime.date(2020, 2, 26)), 'FTNT', ('GE', datetime.date(2020, 2, 24)), ('GE', datetime.date(2020, 2, 13)), 'GE', ('ESS', datetime.date(2020, 2, 20)), 'ESS', ('HLT', datetime.date(2020, 2, 11)), 'HLT', ('MYL', datetime.date(2020, 2, 28)), 'MYL', ('INTU', datetime.date(2020, 2, 24)), 'INTU', ('DD', datetime.date(2020, 2, 14)), 'DD', ('WDC', datetime.date(2020, 2, 11)), 'WDC', ('BIIB', datetime.date(2020, 2, 6)), 'BIIB', ('ZION', datetime.date(2020, 2, 26)), 'ZION', ('APH', datetime.date(2020, 2, 12)), 'APH', ('GL', datetime.date(2020, 1, 31)), ('GL', datetime.date(2020, 2, 27)), 'GL', ('NVDA', datetime.date(2020, 2, 20)), 'NVDA', ('NEM', datetime.date(2020, 2, 20)), 'NEM', ('AMGN', datetime.date(2020, 2, 12)), 'AMGN', ('CTVA', datetime.date(2020, 2, 14)), 'CTVA', ('LEG', datetime.date(2020, 2, 20)), 'LEG', ('PWR', datetime.date(2020, 2, 28)), 'PWR', ('ZTS', datetime.date(2020, 2, 13)), 'ZTS', ('COTY', datetime.date(2020, 2, 5)), 'COTY', ('NOV', datetime.date(2020, 2, 13)), 'NOV', ('AVB', datetime.date(2020, 2, 21)), 'AVB', ('GM', datetime.date(2020, 2, 5)), 'GM', ('WAT', datetime.date(2020, 2, 25)), 'WAT', ('PXD', datetime.date(2020, 2, 24)), 'PXD', ('DE', datetime.date(2020, 2, 5)), ('DE', datetime.date(2020, 2, 4)), ('DE', datetime.date(2020, 2, 7)), ('DE', datetime.date(2020, 2, 27)), ('DE', datetime.date(2020, 2, 20)), ('DE', datetime.date(2020, 2, 19)), ('DE', datetime.date(2020, 2, 13)), ('DE', datetime.date(2020, 2, 12)), ('DE', datetime.date(2020, 2, 14)), ('DE', datetime.date(2020, 2, 26)), ('DE', datetime.date(2020, 2, 21)), ('DE', datetime.date(2020, 2, 25)), ('DE', datetime.date(2020, 2, 10)), ('DE', datetime.date(2020, 2, 11)), ('DE', datetime.date(2020, 2, 28)), ('DE', datetime.date(2020, 2, 18)), ('DE', datetime.date(2020, 2, 24)), ('DE', datetime.date(2020, 2, 6)), 'DE', ('IDXX', datetime.date(2020, 2, 14)), 'IDXX', ('NLSN', datetime.date(2020, 2, 27)), 'NLSN', ('CSCO', datetime.date(2020, 2, 18)), 'CSCO', ('CVS', datetime.date(2020, 2, 18)), 'CVS', ('GD', datetime.date(2020, 2, 10)), 'GD', ('KO', datetime.date(2020, 2, 24)), 'KO', ('AXP', datetime.date(2020, 2, 13)), 'AXP', ('ARE', datetime.date(2020, 2, 4)), 'ARE', ('HAS', datetime.date(2020, 2, 27)), 'HAS', ('NBL', datetime.date(2020, 2, 12)), 'NBL', ('EA', datetime.date(2020, 2, 4)), 'EA', ('LNC', datetime.date(2020, 2, 20)), 'LNC', ('ODFL', datetime.date(2020, 2, 26)), 'ODFL', ('CTXS', datetime.date(2020, 2, 14)), 'CTXS', ('EXC', datetime.date(2020, 2, 11)), 'EXC', ('CPRI', datetime.date(2020, 2, 5)), 'CPRI', ('NWSA', datetime.date(2020, 2, 7)), 'NWSA', ('CBOE', datetime.date(2020, 2, 21)), 'CBOE', ('ZBRA', datetime.date(2020, 2, 13)), 'ZBRA', ('FANG', datetime.date(2020, 2, 27)), 'FANG', ('AMCR', datetime.date(2020, 2, 11)), 'AMCR', ('ETN', datetime.date(2020, 2, 26)), 'ETN', ('XYL', datetime.date(2020, 2, 28)), 'XYL', ('BMY', datetime.date(2020, 2, 24)), 'BMY', ('FMC', datetime.date(2020, 2, 28)), 'FMC', ('ITW', datetime.date(2020, 2, 14)), 'ITW', ('TSN', datetime.date(2020, 2, 6)), 'TSN', ('CPRT', datetime.date(2020, 2, 27)), 'CPRT', ('MTD', datetime.date(2020, 2, 7)), 'MTD', ('TSCO', datetime.date(2020, 2, 20)), 'TSCO', ('SLG', datetime.date(2020, 2, 28)), 'SLG', ('MRO', datetime.date(2020, 2, 20)), 'MRO', ('LH', datetime.date(2020, 2, 28)), 'LH', ('GWW', datetime.date(2020, 2, 20)), 'GWW', ('CL', datetime.date(2020, 2, 21)), 'CL', ('PSA', datetime.date(2020, 2, 25)), 'PSA', ('BKNG', datetime.date(2020, 2, 26)), 'BKNG', ('CB', datetime.date(2020, 2, 27)), 'CB', ('PPG', datetime.date(2020, 2, 20)), 'PPG', ('IP', datetime.date(2020, 2, 19)), 'IP', ('BKR', datetime.date(2020, 2, 13)), 'BKR', ('CTL', datetime.date(2020, 2, 28)), 'CTL', ('M', datetime.date(2020, 2, 19)), ('M', datetime.date(2020, 2, 27)), ('M', datetime.date(2020, 2, 20)), ('M', datetime.date(2020, 2, 21)), 'M', ('J', datetime.date(2020, 2, 27)), ('J', datetime.date(2020, 1, 31)), ('J', datetime.date(2020, 2, 4)), ('J', datetime.date(2020, 2, 7)), 'J', ('EIX', datetime.date(2020, 2, 27)), 'EIX', ('MLM', datetime.date(2020, 2, 21)), 'MLM', ('C', datetime.date(2020, 2, 19)), ('C', datetime.date(2020, 2, 21)), ('C', datetime.date(2020, 2, 28)), 'C', ('D', datetime.date(2020, 2, 28)), 'D', ('DVA', datetime.date(2020, 2, 21)), 'DVA', ('IEX', datetime.date(2020, 2, 21)), 'IEX', ('SRE', datetime.date(2020, 2, 27)), 'SRE', ('AON', datetime.date(2020, 2, 14)), 'AON', ('VRTX', datetime.date(2020, 2, 13)), 'VRTX', ('LUV', datetime.date(2020, 2, 4)), 'LUV', ('ABMD', datetime.date(2020, 2, 6)), 'ABMD', ('RL', datetime.date(2020, 2, 6)), 'RL', ('ECL', datetime.date(2020, 2, 28)), 'ECL', ('EOG', datetime.date(2020, 2, 27)), 'EOG', ('KIM', datetime.date(2020, 2, 25)), 'KIM', ('MCO', datetime.date(2020, 2, 24)), 'MCO', ('UTX', datetime.date(2020, 2, 6)), 'UTX', ('WRK', datetime.date(2020, 1, 31)), 'WRK', ('COP', datetime.date(2020, 2, 18)), 'COP', ('ORLY', datetime.date(2020, 2, 28)), 'ORLY', ('JPM', datetime.date(2020, 2, 25)), 'JPM', ('FLIR', datetime.date(2020, 2, 27)), 'FLIR', ('WRB', datetime.date(2020, 2, 20)), 'WRB', ('AOS', datetime.date(2020, 2, 24)), 'AOS', ('RE', datetime.date(2020, 2, 21)), ('RE', datetime.date(2020, 2, 19)), 'RE', ('NSC', datetime.date(2020, 2, 6)), 'NSC', ('AES', datetime.date(2020, 2, 28)), 'AES', ('HUM', datetime.date(2020, 2, 20)), 'HUM', ('NVR', datetime.date(2020, 2, 19)), 'NVR', ('STT', datetime.date(2020, 2, 20)), 'STT', ('AIV', datetime.date(2020, 2, 24)), 'AIV', ('SCHW', datetime.date(2020, 2, 26)), 'SCHW', ('GLW', datetime.date(2020, 2, 18)), 'GLW'])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a look at some of the key samples from our dictionary\n",
    "print(\"Here are some of the filings we have in our sample:\")\n",
    "sample_filings_corpus.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and cleaning the filings...\n",
      "filing key is  ('VZ', datetime.date(2020, 2, 21))\n",
      "The filing text type is  <class 'str'>\n",
      "filing key is  VZ\n",
      "The filing text type is  <class 'dict'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [39], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe filing text type is \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mtype\u001B[39m(filing_text))\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# extract a list of tokens from the filing corpus\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m tokenized_filing \u001B[38;5;241m=\u001B[39m \u001B[43mtokenize_filing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfiling_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# remove the stop words and digits from the tokenized filing\u001B[39;00m\n\u001B[1;32m     11\u001B[0m tokenized_filing \u001B[38;5;241m=\u001B[39m clean_filing_corpus(tokenized_filing)\n",
      "Cell \u001B[0;32mIn [14], line 4\u001B[0m, in \u001B[0;36mtokenize_filing\u001B[0;34m(filing_corpus)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize_filing\u001B[39m(filing_corpus):\n\u001B[1;32m      3\u001B[0m     filing_tokenizer \u001B[38;5;241m=\u001B[39m RegexpTokenizer(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfiling_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfiling_corpus\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/nltk/tokenize/regexp.py:133\u001B[0m, in \u001B[0;36mRegexpTokenizer.tokenize\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_regexp\u001B[38;5;241m.\u001B[39msplit(text)\n\u001B[1;32m    131\u001B[0m \u001B[38;5;66;03m# If our regexp matches tokens, use re.findall:\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_regexp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfindall\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# tokenize and clean the body of the filings\n",
    "print(\"Tokenizing and cleaning the filings...\")\n",
    "\n",
    "for filing in sample_filings_corpus:\n",
    "    print(\"filing key is \", filing)\n",
    "    filing_text = sample_filings_corpus[filing]\n",
    "    print(\"The filing text type is \", type(filing_text))\n",
    "    # extract a list of tokens from the filing corpus\n",
    "    tokenized_filing = tokenize_filing(filing_text)\n",
    "    # remove the stop words and digits from the tokenized filing\n",
    "    tokenized_filing = clean_filing_corpus(tokenized_filing)\n",
    "    sample_filings_corpus[filing] = tokenized_filing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TEST\n",
    "print(len(sample_filings_corpus))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculating sentiment score of each filing based on the proportion of\n",
    "# negative words present in it based on Loughran and Harvard dictionaries\n",
    "\n",
    "# filings_loughran_neg_score: { (ticker, date) : (loughran_neg_score,\n",
    "# harvard_neg_score) }\n",
    "\n",
    "filings_neg_score = {}\n",
    "\n",
    "for filing in sample_filings_corpus:\n",
    "    list_of_filing_tokens = sample_filings_corpus[filing]\n",
    "    loughran_neg_score = get_sentiment_score(list_of_filing_tokens, \"Loughran\")\n",
    "    harvard_neg_score = get_sentiment_score(list_of_filing_tokens, \"Harvard\")\n",
    "    filings_neg_score[filing] = (loughran_neg_score, harvard_neg_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TEST\n",
    "print(filings_neg_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loading financial data from CRSP database\n",
    "# the data has been preloaded from CRSP in a form of csv file\n",
    "\n",
    "print(\"Loading CRSP data...\")\n",
    "sample_crsp_data = get_crsp_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculating excess return for all companies in our sample\n",
    "\n",
    "print(\"Calculating excess return for all companies in our sample...\")\n",
    "\n",
    "# filings_excess_return : {(ticker, date) : excess_return}\n",
    "filings_excess_return = {}\n",
    "\n",
    "for filing in sample_filings_corpus:\n",
    "    filing_ticker = filing[0]\n",
    "    filing_date = filing[1]\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    datetime_date = datetime.strptime(filing_date, DATE_FORMAT)\n",
    "    filing_permno = get_permno_from_ticker(sp500_sample, filing_ticker)\n",
    "    filing_excess_return = get_excess_return(sample_crsp_data, filing_permno,\n",
    "                                             filing_date)\n",
    "    filings_excess_return[filing] = filing_excess_return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Performing proportion of negative words analysis on the sp500 sample </h2>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
