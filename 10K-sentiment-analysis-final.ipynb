{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "TODO:\n",
    "* add plotting functionality\n",
    "* add global package importing function\n",
    "* Either use the guys web scraping code or trey again with\n",
    "sec-edgar-downloader api\n",
    "* maybe even try to implement the rest of the project on the data you already\n",
    " have, before moving on to downloading an extra data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "\n",
    "# import project packages\n",
    "import wrds\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sec_edgar_downloader import Downloader\n",
    "import time\n",
    "import calendar\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pysentiment2 as ps\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# this function retrieves all sp500 records from the WRDS database\n",
    "# the records are returned in Pandas dataframe format\n",
    "def get_sp500_records():\n",
    "    WRDS_USERNAME = 'denisfench'\n",
    "    # establish connection to the WRDS database\n",
    "    conn = wrds.Connection(wrds_username=WRDS_USERNAME)\n",
    "\n",
    "    ### Get S&P500 Index Membership from CRSP\n",
    "    sp500 = conn.raw_sql(\"\"\"\n",
    "                            select a.*, b.date, b.ret\n",
    "                            from crsp.msp500list as a,\n",
    "                            crsp.msf as b\n",
    "                            where a.permno=b.permno\n",
    "                            and b.date >= a.start and b.date<= a.ending\n",
    "                            and b.date>='01/01/2000'\n",
    "                            order by date;\n",
    "                            \"\"\", date_cols=['start', 'ending', 'date'])\n",
    "\n",
    "    ### Add Other Company Identifiers from CRSP.MSENAMES, such as TICKER, SHRCD, EXCHCD\n",
    "    mse = conn.raw_sql(\"\"\"\n",
    "                            select comnam, ncusip, namedt, nameendt,\n",
    "                            permno, shrcd, exchcd, hsiccd, ticker\n",
    "                            from crsp.msenames\n",
    "                            \"\"\", date_cols=['namedt', 'nameendt'])\n",
    "\n",
    "    # if nameendt is missing then set to today date\n",
    "    mse['nameendt']=mse['nameendt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "    # Merge with SP500 data\n",
    "    sp500_full = pd.merge(sp500, mse, how = 'left', on = 'permno')\n",
    "\n",
    "    # Impose the date range restrictions\n",
    "    sp500_full = sp500_full.loc[(sp500_full.date>=sp500_full.namedt) \\\n",
    "                                & (sp500_full.date<=sp500_full.nameendt)]\n",
    "\n",
    "    ### Add Other Company Identifiers from CRSP.MSENAMES\n",
    "    mse = conn.raw_sql(\"\"\"\n",
    "                            select comnam, ncusip, namedt, nameendt,\n",
    "                            permno, shrcd, exchcd, hsiccd, ticker\n",
    "                            from crsp.msenames\n",
    "                            \"\"\", date_cols=['namedt', 'nameendt'])\n",
    "\n",
    "    # if nameendt is missing then set to today date\n",
    "    mse['nameendt']=mse['nameendt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "    # Merge with SP500 data\n",
    "    sp500_full = pd.merge(sp500, mse, how = 'left', on = 'permno')\n",
    "\n",
    "    # Impose the date range restrictions\n",
    "    sp500_full = sp500_full.loc[(sp500_full.date>=sp500_full.namedt) \\\n",
    "                                & (sp500_full.date<=sp500_full.nameendt)]\n",
    "\n",
    "    ### Add Compustat Identifiers\n",
    "    ccm=conn.raw_sql(\"\"\"\n",
    "                      select gvkey, liid as iid, lpermno as permno,\n",
    "                      linktype, linkprim, linkdt, linkenddt\n",
    "                      from crsp.ccmxpf_linktable\n",
    "                      where substr(linktype,1,1)='L'\n",
    "                      and (linkprim ='C' or linkprim='P')\n",
    "                      \"\"\", date_cols=['linkdt', 'linkenddt'])\n",
    "\n",
    "    # if linkenddt is missing then set to today date\n",
    "    ccm['linkenddt']=ccm['linkenddt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "    # Merge the CCM data with S&P500 data\n",
    "    # First just link by matching PERMNO\n",
    "    sp500ccm = pd.merge(sp500_full, ccm, how='left', on=['permno'])\n",
    "\n",
    "    # Then set link date bounds\n",
    "    sp500ccm = sp500ccm.loc[(sp500ccm['date']>=sp500ccm['linkdt'])\\\n",
    "                            &(sp500ccm['date']<=sp500ccm['linkenddt'])]\n",
    "\n",
    "    # Rearrange columns for final output\n",
    "    sp500ccm = sp500ccm.drop(columns=['namedt', 'nameendt', 'linktype', \\\n",
    "                                      'linkprim', 'linkdt', 'linkenddt'])\n",
    "    sp500ccm = sp500ccm[['date', 'permno', 'comnam', 'ncusip',\\\n",
    "                         'shrcd', 'exchcd', 'hsiccd', 'ticker', \\\n",
    "                         'gvkey', 'iid', 'start', 'ending', 'ret']]\n",
    "\n",
    "    ### Add CIKs and Link with SEC Index Files using CIK\n",
    "    names = conn.raw_sql(\"\"\" select gvkey, cik, sic, naics, gind, gsubind from comp.names \"\"\")\n",
    "\n",
    "    # Merge sp500 constituents table with names table\n",
    "    sp500 = pd.merge(sp500ccm, names, on='gvkey',  how='left')\n",
    "    return sp500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# this function retrieves a subsample of the sp500 records corresponding to\n",
    "# the given dates (YYYY-MM-DD)\n",
    "# EX: get_sp500_records_by_date(sp500, '2020-01-01', '2021-12-31')\n",
    "\n",
    "def get_sp500_records_by_date(sp500_record, start_date, end_date):\n",
    "    new_sp500_sample = sp500_record.loc[start_date <= sp500_record\n",
    "        .date][['date',\n",
    "                                                                    'permno',\n",
    "                                                      'comnam',\n",
    "                                               'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "\n",
    "    new_sp500_sample = new_sp500_sample.loc[sp500_record.date <=\n",
    "                                      end_date][['date',\n",
    "                                                                        'permno',\n",
    "                                                          'comnam',\n",
    "                                                   'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "\n",
    "    return new_sp500_sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# increment the current date by 1 month\n",
    "def get_next_month(date, DATE_FORMAT=\"%Y-%m-%d\", datetime_format=False):\n",
    "    datetime_date = datetime.strptime(date, DATE_FORMAT)\n",
    "    new_date = datetime_date + relativedelta(months=1)\n",
    "    if datetime_format:\n",
    "        return new_date\n",
    "    else:\n",
    "        return new_date.strftime(DATE_FORMAT)\n",
    "\n",
    "\n",
    "# increment the current date by 1 day\n",
    "def get_next_day(date, datetime_format=False):\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    datetime_date = datetime.strptime(date, DATE_FORMAT)\n",
    "    new_date = datetime_date + relativedelta(days=1)\n",
    "    if datetime_format:\n",
    "        return new_date\n",
    "    else:\n",
    "        return new_date.strftime(DATE_FORMAT)\n",
    "\n",
    "\n",
    "# get the last day of the current month\n",
    "def get_end_month_date(curr_date, datetime_format=False):\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    datetime_date = datetime.strptime(curr_date, DATE_FORMAT)\n",
    "    current_date = curr_date\n",
    "    month_end_date = current_date[:8] + str(calendar.monthrange(datetime_date.year,\n",
    "                                                datetime_date\n",
    "                                      .month)[1])\n",
    "    month_end_date = datetime.strptime(month_end_date, DATE_FORMAT)\n",
    "    if datetime_format:\n",
    "        return month_end_date\n",
    "    else:\n",
    "        return month_end_date.strftime(DATE_FORMAT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# this function retrieves all 10-K and 10-Q fillings for a given company for\n",
    "# a given month\n",
    "def get_filing_by_month(ciks, date, dir_name):\n",
    "    num_filings = 0\n",
    "    dl = Downloader(dir_name)\n",
    "    for cik in ciks:\n",
    "        num_filings += dl.get(\"10-Q\", cik, after=date, before=get_next_month\n",
    "        (date))\n",
    "        num_filings += dl.get(\"10-K\", cik, after=date, before=get_next_month\n",
    "        (date))\n",
    "    return num_filings\n",
    "\n",
    "# this function retrieves all 10-K and 10-Q fillings for a given company for\n",
    "# a given day\n",
    "def get_filing_by_day(ciks, date, dir_name):\n",
    "    num_filings = 0\n",
    "    dl = Downloader(dir_name)\n",
    "    for cik in ciks:\n",
    "        num_filings += dl.get(\"10-Q\", cik, after=date, before=get_next_day\n",
    "        (date))\n",
    "        num_filings += dl.get(\"10-K\", cik, after=date, before=get_next_day\n",
    "        (date))\n",
    "    return num_filings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# this function retrieves all S&P 500 10-Q or 10-K filings for a given month,\n",
    "# if such a filing exists for a given company and writes them to a given directory\n",
    "# the date should be passed in the following format: YYYY-MM-DD\n",
    "# the company lookup is based in the cik number by default\n",
    "def get_all_sp_filings_by_month(sp500_collection, date, dir_name,\n",
    "                                identifier='cik'):\n",
    "    sp500_ids = []\n",
    "    num_filings = 0\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    current_date = date\n",
    "    month_end_date = get_end_month_date(current_date, datetime_format=True)\n",
    "\n",
    "    while datetime.strptime(current_date, DATE_FORMAT) <= month_end_date:\n",
    "        sp500_companies = sp500_collection.loc[sp500_collection.date ==\n",
    "                                               current_date][['date',\n",
    "                                                                        'permno',\n",
    "                                                          'comnam',\n",
    "                                                   'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "\n",
    "        sp500_ids.extend(sp500_companies[identifier].tolist())\n",
    "        current_date = get_next_day(current_date)\n",
    "\n",
    "    num_filings += get_filing_by_month(sp500_ids,\n",
    "                                       current_date,\n",
    "                                       dir_name)\n",
    "\n",
    "    return num_filings\n",
    "\n",
    "# this function retrieves all S&P 500 10-Q or 10-K filings for a given day,\n",
    "# if such a filing exists for a given company and writes them to a given directory\n",
    "# the date should be passed in the following format: YYYY-MM-DD\n",
    "def get_all_sp_filings_by_day(sp500_collection, date, dir_name, identifier='cik'):\n",
    "    num_filings = 0\n",
    "    sp500_companies = sp500_collection.loc[sp500_collection.date == date][['date',\n",
    "                                                                    'permno',\n",
    "                                                      'comnam',\n",
    "                                               'ncusip', 'gvkey', 'iid', 'cik', 'ticker', 'sic', 'naics']]\n",
    "    sp500_tickers = sp500_companies[identifier]\n",
    "\n",
    "    for ticker in sp500_tickers:\n",
    "        num_filings += get_filing_by_day(ticker, date, dir_name)\n",
    "\n",
    "    return num_filings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# this function retrieves all S&P 500 10-Q and 10-K filings for a given period\n",
    "def get_sp500_filings(sp500_collection, filings_start_date, filings_end_date,\n",
    "                      dir_name):\n",
    "\n",
    "    print(\"Retrieving all the 10-Q and 10-K filings starting from \",\n",
    "           filings_start_date, \" until \", filings_end_date)\n",
    "\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    current_date = filings_start_date\n",
    "    num_filings = 0\n",
    "    while datetime.strptime(current_date, DATE_FORMAT) <= datetime.strptime\\\n",
    "                (filings_end_date, DATE_FORMAT):\n",
    "        num_filings += get_all_sp_filings_by_month(sp500_collection, current_date,\n",
    "                                                   dir_name)\n",
    "        current_date = get_next_month(current_date)\n",
    "\n",
    "        print(\"Successfully retrieved \" + str(num_filings) + \" for \" + str\n",
    "        (current_date))\n",
    "\n",
    "    return num_filings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# this function retrieves all tickers for all companies from a given directory\n",
    "def get_all_filing_tickers(dir_name):\n",
    "    files = os.listdir(dir_name + \"/sec-edgar-filings\")\n",
    "    return files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# this function extracts the filing date, given the filing date line from the\n",
    "# filing document\n",
    "def extract_filing_date(filed_as_of_date_line):\n",
    "    DATE_FORMAT = \"%Y%m%d\"\n",
    "    date = \"\"\n",
    "    for ch in filed_as_of_date_line:\n",
    "        if ch.isdigit():\n",
    "            date += ch\n",
    "    return datetime.strptime(date, DATE_FORMAT).date()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "FILING_DETAILS = \"filing-details.html\"\n",
    "FULL_SUBMISSION = \"full-submission.txt\"\n",
    "FILED_AS_OF_DATE_IDX = 7\n",
    "\n",
    "# this function will search for a filing in a given directory based on the\n",
    "# given ticker\n",
    "# it will return a dictionary with a ticker and the date of filing as the key\n",
    "# and filing text as the value\n",
    "# currently the function doesn't indicate whether the filing is a 10-Q or 10-K\n",
    "def get_filings_from_ticker(ticker, dir_name, sec_filings):\n",
    "    for root, dirs, files in os.walk(dir_name + \"/sec-edgar-filings/\" + ticker):\n",
    "        for f in files:\n",
    "            if FILING_DETAILS in f:\n",
    "                with open(root + \"/\" + f) as fp:\n",
    "                    soup = BeautifulSoup(fp, \"html.parser\")\n",
    "                    line_idx = 0\n",
    "                    with open(root + \"/\" + FULL_SUBMISSION) as fs_fp:\n",
    "                        while line_idx < FILED_AS_OF_DATE_IDX:\n",
    "                            line_idx += 1\n",
    "                            fs_fp.readline()\n",
    "                        date = extract_filing_date(fs_fp.readline())\n",
    "                    sec_filings[(ticker, date)] = soup.get_text()\n",
    "    return sec_filings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# this function sets up SSL context for downloading packages from nltk library\n",
    "def create_ssl_context():\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# a function that tokenizes 10-K or 10-Q corpus that\n",
    "def tokenize_filing(filing_corpus):\n",
    "    filing_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return filing_tokenizer.tokenize(filing_corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# a function to remove the stop words from the filings corpus\n",
    "def filter_out_stopwords(tokenized_filing_corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filing_corpus_filtered = [word for word in tokenized_filing_corpus if not \\\n",
    "        word.lower() in stop_words]\n",
    "    return filing_corpus_filtered\n",
    "\n",
    "\n",
    "# a function that filters out numbers from a filing corpus\n",
    "def filter_out_numbers(filing_corpus):\n",
    "    return [token for token in filing_corpus if not (token.isdigit()\n",
    "                                         or token[0] == '-' and token[1:].isdigit())]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on 10-Q and 10-K filing documents\n",
    "\n",
    "# this function calculates the sentiment score of the text based on a\n",
    "# sentiment dictionary (Loughran or Harvard), passed in as a parameter\n",
    "def get_sentiment_score(text, dictionary):\n",
    "\n",
    "    # get the Harvard general sentiment dictionary\n",
    "    hiv4 = ps.HIV4()\n",
    "\n",
    "    # get the Loughran and McDonald dictionary\n",
    "    lm = ps.LM()\n",
    "\n",
    "    if dictionary == \"Harvard\":\n",
    "        return hiv4.get_sentiment_score(text)\n",
    "\n",
    "    if dictionary == \"Loughran\":\n",
    "        return lm.get_sentiment_score(text)\n",
    "\n",
    "    raise ValueError(\"The dictionary could not be found.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# this function retrieves Value-Weighted Return (including distributions) and\n",
    "# Holding Period Return from the CRSP database\n",
    "def get_crsp_data():\n",
    "    CRSP_DATA_DIR = \"CRSP-data/\"\n",
    "    crsp_df = pd.read_csv(CRSP_DATA_DIR + \"crsp.csv\", parse_dates=['date'])\n",
    "    return crsp_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# this function calculates excess return using Value-Weighted Return (including distributions) and\n",
    "# Holding Period Return from the CRSP database\n",
    "def get_excess_return(crsp_dataframe, permno, date):\n",
    "    excess_return_df = crsp_dataframe[crsp_dataframe['PERMNO'] == permno]\n",
    "    excess_return_df =  excess_return_df[crsp_dataframe['date'] == date]\n",
    "    excess_return = float(excess_return_df['vwretd']) - \\\n",
    "                    float(excess_return_df['RET'])\n",
    "    return excess_return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "DICTIONARY_DIR = \"dictionaries/\"\n",
    "\n",
    "# this function constructs a sentiment dictionary based on the Loughran\n",
    "# McDonald and Harvard dictionaries\n",
    "# return: {Term: [str], Sentiment: [int], Term_Count: [int]}\n",
    "# where Term count is the number of times the term occurs within the corpus\n",
    "def construct_sentiment_dict(base_dict=\"Loughran\"):\n",
    "    if base_dict == \"Loughran\":\n",
    "        df = pd.read_csv(DICTIONARY_DIR +\n",
    "                         \"Loughran-McDonald_MasterDictionary_1993-2021.csv\",\n",
    "                         header=0, usecols=['Word', 'Negative', 'Positive'])\n",
    "    else:\n",
    "        df = pd.read_csv(DICTIONARY_DIR + \"Harvard.csv\", header=0, usecols=['Word', 'Negative', 'Positive'])\n",
    "\n",
    "    df = df[(df['Negative'] != 0) | (df['Positive'] != 0)]\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "            Word  Negative  Positive\n9        ABANDON      2009         0\n10     ABANDONED      2009         0\n11    ABANDONING      2009         0\n12   ABANDONMENT      2009         0\n13  ABANDONMENTS      2009         0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Negative</th>\n      <th>Positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9</th>\n      <td>ABANDON</td>\n      <td>2009</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>ABANDONED</td>\n      <td>2009</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ABANDONING</td>\n      <td>2009</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>ABANDONMENT</td>\n      <td>2009</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ABANDONMENTS</td>\n      <td>2009</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing Loughran sentiment dictionary\n",
    "lr_dict = construct_sentiment_dict()\n",
    "lr_dict.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# This function constructs a term dictionary based on a given document: tokenized filing list\n",
    "# return: {Term: [str], Term_Count: [int]}\n",
    "# where Term count is the number of times the term occurs within the document\n",
    "def construct_term_dict(document):\n",
    "    doc_dict = {}\n",
    "    for token in document:\n",
    "        if token in doc_dict:\n",
    "            doc_dict[token] += 1\n",
    "        else:\n",
    "            doc_dict[token] = 1\n",
    "    return doc_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# This function calculates a document negativity score based on the\n",
    "# proportion of negative words in the document\n",
    "# the negativity of the term is determined based on a given dictionary\n",
    "# (Loughran or Harvard)\n",
    "def compute_negativity_score_based_on_proportion(text,\n",
    "                                                 sentiment_dict=\"Loughran\"):\n",
    "    sentiment_score = get_sentiment_score(text, sentiment_dict)\n",
    "    num_negative = sentiment_score['Negative']\n",
    "    return num_negative / len(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# *** TO BE REMOVED ***\n",
    "\n",
    "# This function calculates a document negativity score based on the\n",
    "# TFIDF score for the document\n",
    "# TFIDF:\n",
    "# TF (Term Frequency) (t, d) = (# Occurrences of the word in a document / # Total words in a document )\n",
    "# IDF (Inverse Document Frequency) (t, D) = log ( (N (number of documents in a collection) / (# Documents containing the term t))\n",
    "# TFIDF (t, d, D) = TF (t, d) * IDF (t, D)\n",
    "# the function takes document dictionary, containing terms and their\n",
    "# frequencies for a given document; the sentiment dictionary, either Loughran\n",
    "# or Harvard, containing token sentiment rating as well as the document term\n",
    "# count for the entire corpus; the number of documents in the collection and\n",
    "# the number of tokens within a given document\n",
    "def get_tfidf_negativity_score(doc_dict, sentiment_dict, doc_term_freq,\n",
    "                               collection_size):\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# This function calculates TFIDF score for the filings based on the sentiment\n",
    "# dictionary vocabulary using SciKit Learn\n",
    "def calc_tfidf_score(sentiment_dict, corpus):\n",
    "    vectorizer = TfidfVectorizer(vocabulary=sentiment_dict.keys(),\n",
    "                                 stop_words=\"english\")\n",
    "    vectors = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    dense_list = dense.tolist()\n",
    "    tfidf_df = pd.DataFrame(dense_list, columns=feature_names)\n",
    "    return tfidf_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# this function loads all the filings from a given directory, which\n",
    "# constitute an entire corpus of documents\n",
    "def load_filings_corpus(corpus_root_dir):\n",
    "    FILING_DETAILS = \"filing-details.html\"\n",
    "    sec_filings_corpus = []\n",
    "    for root, dirs, files in os.walk(corpus_root_dir + \"/sec-edgar-filings/\"):\n",
    "        for f in files:\n",
    "            if FILING_DETAILS in f:\n",
    "                with open(root + \"/\" + f) as file_ptr:\n",
    "                    bs = BeautifulSoup(file_ptr, \"html.parser\")\n",
    "                    sec_filings_corpus.append(bs.get_text())\n",
    "    return sec_filings_corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>SENTIMENT ANALYSIS OF 10-Qs and 10-Ks for S&P 500 companies for 2020,\n",
    "2021<h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# getting all S&P 500 records from the WRDS database\n",
    "\n",
    "sp500_all = get_sp500_records()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "             date   permno               comnam    ncusip   gvkey iid  \\\n120131 2020-01-31  34746.0  FIFTH THIRD BANCORP  31677310  004640  01   \n120132 2020-01-31  75591.0            IDEX CORP  45167R10  015267  01   \n120133 2020-01-31  14541.0     CHEVRON CORP NEW  16676410  002991  01   \n120134 2020-01-31  14593.0            APPLE INC  03783310  001690  01   \n120135 2020-01-31  75341.0     DUKE REALTY CORP  26441150  013510  01   \n\n               cik ticker   sic   naics  \n120131  0000035527   FITB  6020  522110  \n120132  0000832101    IEX  3561  333914  \n120133  0000093410    CVX  2911  324110  \n120134  0000320193   AAPL  3663  334220  \n120135  0000783280    DRE  6798  531120  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>permno</th>\n      <th>comnam</th>\n      <th>ncusip</th>\n      <th>gvkey</th>\n      <th>iid</th>\n      <th>cik</th>\n      <th>ticker</th>\n      <th>sic</th>\n      <th>naics</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>120131</th>\n      <td>2020-01-31</td>\n      <td>34746.0</td>\n      <td>FIFTH THIRD BANCORP</td>\n      <td>31677310</td>\n      <td>004640</td>\n      <td>01</td>\n      <td>0000035527</td>\n      <td>FITB</td>\n      <td>6020</td>\n      <td>522110</td>\n    </tr>\n    <tr>\n      <th>120132</th>\n      <td>2020-01-31</td>\n      <td>75591.0</td>\n      <td>IDEX CORP</td>\n      <td>45167R10</td>\n      <td>015267</td>\n      <td>01</td>\n      <td>0000832101</td>\n      <td>IEX</td>\n      <td>3561</td>\n      <td>333914</td>\n    </tr>\n    <tr>\n      <th>120133</th>\n      <td>2020-01-31</td>\n      <td>14541.0</td>\n      <td>CHEVRON CORP NEW</td>\n      <td>16676410</td>\n      <td>002991</td>\n      <td>01</td>\n      <td>0000093410</td>\n      <td>CVX</td>\n      <td>2911</td>\n      <td>324110</td>\n    </tr>\n    <tr>\n      <th>120134</th>\n      <td>2020-01-31</td>\n      <td>14593.0</td>\n      <td>APPLE INC</td>\n      <td>03783310</td>\n      <td>001690</td>\n      <td>01</td>\n      <td>0000320193</td>\n      <td>AAPL</td>\n      <td>3663</td>\n      <td>334220</td>\n    </tr>\n    <tr>\n      <th>120135</th>\n      <td>2020-01-31</td>\n      <td>75341.0</td>\n      <td>DUKE REALTY CORP</td>\n      <td>26441150</td>\n      <td>013510</td>\n      <td>01</td>\n      <td>0000783280</td>\n      <td>DRE</td>\n      <td>6798</td>\n      <td>531120</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieving all S&P 500 records for the two-year period (from '2020-01-01'\n",
    "# to '2021-12-31')\n",
    "\n",
    "sp500_sample = get_sp500_records_by_date(sp500_all, \"2020-01-01\", \"2021-12-31\")\n",
    "sp500_sample.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving all the 10-Q and 10-K filings starting from  2020-01-01  until  2021-12-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennisfenchenko/NYU-Fall-2022/nlp-finance/10k-sentiment-analysis/venv/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get a sample of S&P 500 filings for a 2-year period\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2021-12-31\"\n",
    "sp_filings_dir_name = 'sp500_filings_sample'\n",
    "num_filings_sample = get_sp500_filings(sp500_sample, start_date, end_date, sp_filings_dir_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}